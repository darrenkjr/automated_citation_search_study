{
  "cells": [
    {
      "cell_type": "raw",
      "id": "bb16cd16",
      "metadata": {},
      "source": [
        "---\n",
        "bibliography: references.bib\n",
        "format: \n",
        "    # pdf: \n",
        "    #     echo: False\n",
        "    #     toc : True\n",
        "    docx: \n",
        "        reference-doc : custom-reference-doc.docx\n",
        "        echo: false\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0d117d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd \n",
        "import ast\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ast \n",
        "from IPython.display import Markdown\n",
        "from tabulate import tabulate\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393fc295",
      "metadata": {},
      "source": [
        "# Read Dataset Characterstics and Study Results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2f817945",
      "metadata": {},
      "outputs": [],
      "source": [
        "excel_path_campbell_oa = 'Dataset\\Results\\sr_samples_campbell_openalex.xlsx'\n",
        "excel_path_cee_oa = 'Dataset\\Results\\sr_samples_cee_openalex.xlsx'\n",
        "excel_path_cochrane_oa = 'Dataset\\Results\\sr_samples_cochrane_openalex.xlsx'\n",
        "\n",
        "excel_path_campbell_ss = 'Dataset\\Results\\sr_samples_campbell_semanticscholar.xlsx'\n",
        "excel_path_cee_ss = 'Dataset\\Results\\sr_samples_cee_semanticscholar.xlsx'\n",
        "excel_path_cochrane_ss = 'Dataset\\Results\\sr_samples_cochrane_semanticscholar.xlsx'\n",
        "\n",
        "#sr data\n",
        "original_sr_campbell_oa = pd.read_excel(excel_path_campbell_oa , sheet_name='sys_rev_data_oa')\n",
        "original_sr_cee_oa = pd.read_excel(excel_path_cee_oa , sheet_name='sys_rev_data_oa')\n",
        "original_sr_cochrane_oa = pd.read_excel(excel_path_cochrane_oa , sheet_name='sys_rev_data_oa')\n",
        "\n",
        "original_sr_campbell_ss = pd.read_excel(excel_path_campbell_ss , sheet_name='sys_rev_data_ss')\n",
        "original_sr_cee_ss = pd.read_excel(excel_path_cee_ss , sheet_name='sys_rev_data_ss')\n",
        "original_sr_cochrane_ss = pd.read_excel(excel_path_cochrane_ss , sheet_name='sys_rev_data_ss')\n",
        "\n",
        "# original SR data \n",
        "baseline_review_data_oa = pd.concat([original_sr_cee_oa, original_sr_campbell_oa, original_sr_cochrane_oa],ignore_index=True)\n",
        "baseline_review_data_ss = pd.concat([original_sr_cee_ss, original_sr_campbell_ss, original_sr_cochrane_ss],ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a8c1b8",
      "metadata": {},
      "source": [
        "### Read Included Articles / Review Question Characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "098c3d65",
      "metadata": {},
      "outputs": [],
      "source": [
        "#included article data\n",
        "included_article_campbell_oa = pd.read_excel(excel_path_campbell_oa , sheet_name='sys_rev_included_data_oa')\n",
        "included_article_cee_oa = pd.read_excel(excel_path_cee_oa , sheet_name='sys_rev_included_data_oa')\n",
        "included_article_cochrane_oa = pd.read_excel(excel_path_cochrane_oa , sheet_name='sys_rev_included_data_oa')\n",
        "\n",
        "included_article_campbell_ss = pd.read_excel(excel_path_campbell_ss , sheet_name='sys_rev_included_data_ss')\n",
        "included_article_cee_ss = pd.read_excel(excel_path_cee_ss , sheet_name='sys_rev_included_data_ss')\n",
        "included_article_cochrane_ss = pd.read_excel(excel_path_cochrane_ss , sheet_name='sys_rev_included_data_ss')\n",
        "\n",
        "\n",
        "def consolidate_included_article_ids(row):\n",
        "    return row['included_doi'] or row['included_pmid'] or row['included_mag_id']\n",
        "\n",
        "included_article_campbell_oa['consolidated_ids'] = included_article_campbell_oa.apply(consolidate_included_article_ids, axis=1)\n",
        "included_article_cee_oa['consolidated_ids'] = included_article_cee_oa.apply(consolidate_included_article_ids, axis=1)\n",
        "included_article_cochrane_oa['consolidated_ids'] = included_article_cochrane_oa.apply(consolidate_included_article_ids, axis=1)\n",
        "\n",
        "included_article_campbell_ss['consolidated_ids'] = included_article_campbell_ss.apply(consolidate_included_article_ids, axis=1)\n",
        "included_article_cee_ss['consolidated_ids'] = included_article_cee_ss.apply(consolidate_included_article_ids, axis=1)\n",
        "included_article_cochrane_ss['consolidated_ids'] = included_article_cochrane_ss.apply(consolidate_included_article_ids, axis=1)\n",
        "\n",
        "included_campbell_oa = included_article_campbell_oa.query('not no_data_no_id.isna()')\n",
        "included_campbell_ss = included_article_campbell_ss.query('not no_data_no_id.isna()')\n",
        "\n",
        "included_cee_oa = included_article_cee_oa.query('not no_data_no_id.isna()')\n",
        "included_cee_ss = included_article_cee_ss.query('not no_data_no_id.isna()')\n",
        "\n",
        "included_cochrane_oa = included_article_cochrane_oa.query('not no_data_no_id.isna()')\n",
        "included_cochrane_ss = included_article_cochrane_ss.query('not no_data_no_id.isna()')\n",
        "\n",
        "included_all_oa = pd.concat([included_cee_oa, included_campbell_oa, included_cochrane_oa], ignore_index=True)\n",
        "included_all_ss = pd.concat([included_cee_ss, included_campbell_ss, included_cochrane_ss], ignore_index = True)\n",
        "\n",
        "included_all_oa['no_id'] = np.where(\n",
        "    (included_all_oa['no_data_no_id'] == 1) & (included_all_oa['no_data_from_api'] == 0),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "included_all_oa.drop(columns=['not_retrieved'], inplace=True)\n",
        "\n",
        "# For the DataFrame `included_all_ss`\n",
        "included_all_ss['no_id'] = np.where(\n",
        "    (included_all_ss['no_data_no_id'] == 1) & (included_all_ss['no_data_from_api'] == 0),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "included_all_ss.drop(columns=['not_retrieved'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ed1d30e",
      "metadata": {},
      "source": [
        "# RESULTS TABLE 3 : Median number of included articles (IQR) and average intracluster semantic similarity (+/- SD) for Systematic Reviews in each Source Database, and All Reviews in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-included_articles",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-included_articles\n",
        "#| tbl-cap: 'Median number of included articles (IQR) and average intracluster semantic similarity (+/- SD) for Systematic Reviews in each Source Database, and All Reviews in the Dataset'\n",
        "\n",
        "original_sr_included_merge = pd.merge(included_all_oa, baseline_review_data_oa, how='left', left_on='original_sr_id', right_on = 'id', suffixes=('_included', '_original_sr'))\n",
        "# Define IQR function\n",
        "def iqr(x):\n",
        "    return x.quantile(0.75) - x.quantile(0.25)\n",
        "\n",
        "\n",
        "    \n",
        "# Initial grouping and count\n",
        "col_interest = ['source_database', 'original_sr_id', 'intra_cluster_similarity', 'consolidated_ids']\n",
        "col_groupby = ['source_database', 'original_sr_id', 'intra_cluster_similarity']\n",
        "included_summary = original_sr_included_merge[col_interest].groupby(col_groupby).agg({'consolidated_ids': 'count'}).reset_index()\n",
        "included_summary.rename(columns={'consolidated_ids': 'included_count'}, inplace=True)\n",
        "\n",
        "# Second-level grouping with median, IQR, mean, and std calculations\n",
        "included_summary_database = included_summary.groupby(['source_database']).agg({'included_count': ['median', iqr], 'intra_cluster_similarity': ['median', iqr]}).reset_index()\n",
        "\n",
        "# Flatten the MultiIndex\n",
        "included_summary_database.columns = ['_'.join(col).rstrip('_') for col in included_summary_database.columns.values]\n",
        "\n",
        "# Create a summary row based on the included_summary DataFrame\n",
        "new_row_data = {\n",
        "    'source_database': 'All Reviews',\n",
        "    'included_count_median': included_summary['included_count'].median(),\n",
        "    'included_count_iqr': iqr(included_summary['included_count']),\n",
        "    'intra_cluster_similarity_median': included_summary['intra_cluster_similarity'].median(),\n",
        "    'intra_cluster_similarity_iqr': iqr(included_summary['intra_cluster_similarity'])\n",
        "}\n",
        "\n",
        "# Append the new summary row\n",
        "included_summary_database = pd.concat([included_summary_database, pd.DataFrame([new_row_data])], ignore_index=True)\n",
        "\n",
        "# Function to format rows for the Markdown table\n",
        "def format_row(row):\n",
        "    median_iqr_count = f\"{row['included_count_median']} ({row['included_count_iqr']})\"\n",
        "    median_iqr_intrasim= f\"{row['intra_cluster_similarity_median']:.3f} ({row['intra_cluster_similarity_iqr']:.3f})\"\n",
        "    return f\"| {row['source_database']} | {median_iqr_count} | {median_iqr_intrasim} |\"\n",
        "\n",
        "# Create Markdown table headers\n",
        "md_table = \"|Source Database|Number of Included Articles (IQR)|Intra-Cluster Similarity (IQR)|\\n\"\n",
        "md_table += \"|-----------------|---------------------------------|---------------------------------------------|\\n\"\n",
        "\n",
        "# Add rows to the Markdown table\n",
        "for idx, row in included_summary_database.iterrows():\n",
        "    md_table += format_row(row) + '\\n'\n",
        "\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cf6430",
      "metadata": {},
      "source": [
        "### Theoretical Maximum Achievable Recall by Automated Citation Searching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bf7bfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "#this may be come a supplementary table as it oulines tehoretica lrecall for all reviews - but we will decide on that later \n",
        "\n",
        "def calculate_retrieval_stats(df, original_sr_id, included_num):\n",
        "    stats = {}\n",
        "    mask = df['original_sr_id'] == original_sr_id\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    stats['no_id_num'] = filtered_df['no_id'].sum()\n",
        "    stats['no_data_from_api_num'] = filtered_df['no_data_from_api'].sum()\n",
        "    stats['no_id_no_api_num'] = filtered_df['no_data_no_id'].sum()\n",
        "    stats['api_success_retrieved_num'] = sum(filtered_df['no_data_no_id'] == 0)\n",
        "    stats['retrievable_id_num'] = included_num - stats['no_id_num']\n",
        "    \n",
        "\n",
        "    if included_num != 0:\n",
        "        stats['no_id_percentage'] = round((stats['no_id_num'] / included_num) * 100, 1)\n",
        "        stats['no_data_api_percentage'] = round((stats['no_data_from_api_num'] / included_num) * 100, 1)\n",
        "        stats['api_success_retrieved_percentage'] = round((stats['api_success_retrieved_num'] / included_num) * 100, 1)\n",
        "        stats['retrievable_id_percentage'] = round((stats['retrievable_id_num'] / included_num) * 100, 1)\n",
        "\n",
        "        stats['no_id_percentage_with_counts'] = f\"{stats['no_id_percentage']} % (n={stats['no_id_num']})\"\n",
        "        stats['retrievable_id_percentage_with_counts'] = f\"{stats['retrievable_id_percentage']} % (n={stats['retrievable_id_num']})\"\n",
        "        stats['no_data_api_percentage_with_counts'] = f\"{stats['no_data_api_percentage']} % (n={stats['no_data_from_api_num']})\"\n",
        "        stats['success_retrieved_percentage_with_counts'] = f\"{stats['api_success_retrieved_percentage']} % (n={stats['api_success_retrieved_num']})\"\n",
        "    else:\n",
        "        stats['no_id_percentage'] = 0\n",
        "        stats['no_data_api_percentage'] = 0\n",
        "        stats['success_retrieved_percentage'] = 0\n",
        "        stats['no_id_percentage_with_counts'] = f\"{stats['no_id_percentage']}% (n={stats['no_id_num']})\"\n",
        "        stats['no_data_api_percentage_with_counts'] = f\"{stats['no_data_api_percentage']}% (n={stats['no_data_from_api_num']})\"\n",
        "        stats['success_retrieved_percentage_with_counts'] = f\"{stats['success_retrieved_percentage']}% (n={stats['success_retrieved_num']})\"\n",
        "        stats['retrievable_id_percentage_with_counts'] = f\"{stats['retrievable_id_percentage']}% (n={stats['retrievable_id_num']})\"\n",
        "\n",
        "    return stats\n",
        "\n",
        "original_sr_all = baseline_review_data_oa.copy()\n",
        "\n",
        "for original_sr_id in original_sr_all['id'].unique():\n",
        "    mask_oa = included_all_oa['original_sr_id'] == original_sr_id\n",
        "    mask_ss = included_all_ss['original_sr_id'] == original_sr_id\n",
        "    included_num = len(included_all_oa[mask_oa])\n",
        "\n",
        "    stats_oa = calculate_retrieval_stats(included_all_oa, original_sr_id, included_num)\n",
        "    stats_ss = calculate_retrieval_stats(included_all_ss, original_sr_id, included_num)\n",
        "\n",
        "    original_sr_all.loc[original_sr_all['id'] == original_sr_id, 'included_num'] = included_num\n",
        "\n",
        "    for prefix, stats in {'oa': stats_oa, 'ss': stats_ss}.items():\n",
        "        for key, value in stats.items():\n",
        "            original_sr_all.loc[original_sr_all['id'] == original_sr_id, f\"{key}_{prefix}\"] = value\n",
        "\n",
        "extract_col = ['source_database', 'included_num', 'retrievable_id_percentage_with_counts_ss','success_retrieved_percentage_with_counts_oa','success_retrieved_percentage_with_counts_ss']\n",
        "original_sr_performance_df = original_sr_all[extract_col].copy()\n",
        "original_sr_performance_df.rename(columns = {'retrievable_id_percentage_with_counts_ss':'retrievable_id_percentage_with_counts'}, inplace = True)\n",
        "\n",
        "extract_col_new = ['source_database', 'included_num', 'retrievable_id_percentage_with_counts','success_retrieved_percentage_with_counts_oa','success_retrieved_percentage_with_counts_ss']\n",
        "\n",
        "## change column names \n",
        "target_col_names = ['Source Database', 'Number of Included Articles', 'Percentage of Included Articles with Retrievable IDs', 'Maximum Theoretical Recall (OpenAlex)', 'Maximum Theoretical Recall (Semantic Scholar)']\n",
        "col_name_mapping = dict(zip(extract_col_new, target_col_names))\n",
        "original_sr_performance_df.rename(columns = col_name_mapping, inplace=True)\n",
        "\n",
        "\n",
        "def iqr(x): \n",
        "    return round(x.quantile(0.75) - x.quantile(0.25),2)\n",
        "\n",
        "def percentage_achieve_100(x): \n",
        "    return round((x.sum() / len(x)) * 100, 1)\n",
        "raw_col = ['source_database', 'included_num', 'retrievable_id_percentage_oa','api_success_retrieved_percentage_oa','api_success_retrieved_percentage_ss']\n",
        "# Prepare raw_data DataFrame\n",
        "raw_data = original_sr_all[raw_col].copy()\n",
        "raw_data['retrieved_id_100'] = (raw_data['retrievable_id_percentage_oa'] == 100).astype(int)\n",
        "raw_data['retrieved_api_100_oa'] = (raw_data['api_success_retrieved_percentage_oa'] == 100).astype(int)\n",
        "raw_data['retrieved_api_100_ss'] = (raw_data['api_success_retrieved_percentage_ss'] == 100).astype(int)\n",
        "\n",
        "# Prepare the summary DataFrame\n",
        "agg_columns = {\n",
        "    'included_num': ['median', iqr],\n",
        "    'retrievable_id_percentage_oa': ['median', iqr],\n",
        "    'api_success_retrieved_percentage_oa': ['median', iqr],\n",
        "    'api_success_retrieved_percentage_ss': ['median', iqr],\n",
        "    'retrieved_id_100': [percentage_achieve_100, 'sum'],\n",
        "    'retrieved_api_100_oa': [percentage_achieve_100, 'sum'],\n",
        "    'retrieved_api_100_ss': [percentage_achieve_100, 'sum']\n",
        "}\n",
        "\n",
        "original_sr_performance_df_summary = (\n",
        "    raw_data\n",
        "    .groupby(['source_database'])\n",
        "    .agg(agg_columns)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Flatten the MultiIndex for columns\n",
        "original_sr_performance_df_summary.columns = [\n",
        "    '_'.join(col).rstrip('_') \n",
        "    for col in original_sr_performance_df_summary.columns.values\n",
        "]\n",
        "\n",
        "# Prepare summary data for 'All Reviews'\n",
        "extra_raw_all_reviews = {\n",
        "    'source_database': 'All Reviews',\n",
        "    'included_num_median': raw_data['included_num'].median(),\n",
        "    'included_num_iqr': iqr(raw_data['included_num']),\n",
        "    'retrievable_id_percentage_oa_median': raw_data['retrievable_id_percentage_oa'].median(),\n",
        "    'retrievable_id_percentage_oa_iqr': iqr(raw_data['retrievable_id_percentage_oa']),\n",
        "    'api_success_retrieved_percentage_oa_median': raw_data['api_success_retrieved_percentage_oa'].median(),\n",
        "    'api_success_retrieved_percentage_oa_iqr': iqr(raw_data['api_success_retrieved_percentage_oa']),\n",
        "    'api_success_retrieved_percentage_ss_median': raw_data['api_success_retrieved_percentage_ss'].median(),\n",
        "    'api_success_retrieved_percentage_ss_iqr': iqr(raw_data['api_success_retrieved_percentage_ss']),\n",
        "    'retrieved_id_100_percentage_achieve_100': percentage_achieve_100(raw_data['retrieved_id_100']),\n",
        "    'retrieved_api_100_oa_percentage_achieve_100': percentage_achieve_100(raw_data['retrieved_api_100_oa']),\n",
        "    'retrieved_api_100_ss_percentage_achieve_100': percentage_achieve_100(raw_data['retrieved_api_100_ss']),\n",
        "    'retrieved_id_100_sum': raw_data['retrieved_id_100'].sum(),\n",
        "    'retrieved_api_100_oa_sum': raw_data['retrieved_api_100_oa'].sum(),\n",
        "    'retrieved_api_100_ss_sum': raw_data['retrieved_api_100_ss'].sum()\n",
        "}\n",
        "\n",
        "all_review_summary = pd.DataFrame([extra_raw_all_reviews])\n",
        "\n",
        "# Concatenate the summary and all_review_summary DataFrames\n",
        "original_sr_performance_df_summary = pd.concat(\n",
        "    [original_sr_performance_df_summary, all_review_summary], \n",
        "    ignore_index=True\n",
        ")\n",
        "custom_order_dict = {'All Reviews' : 1, 'CEE' : 2, 'Campbell' : 3, 'Cochrane' : 4}\n",
        "\n",
        "# Sort DataFrame using a custom key function\n",
        "original_sr_performance_df_summary  = original_sr_performance_df_summary.sort_values(\n",
        "    by='source_database',\n",
        "    key=lambda x: x.map(custom_order_dict)\n",
        ").reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be1acaa9",
      "metadata": {},
      "source": [
        "# Unused Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-percentage_of_review_100recall",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-percentage_of_review_100recall\n",
        "#| tbl-cap: 'Percentage of Reviews with 100% Theoretical Recall by Automated Citation Searching for each Systematic Review Source Database, by API (OpenAlex / Semantic Scholar)'\n",
        "\n",
        "def format_row(row): \n",
        "    percentage_num_retrieved_id = f\"{row['retrieved_id_100_percentage_achieve_100']}% (n={row['retrieved_id_100_sum']})\"\n",
        "    percentage_num_retrieved_id= f\"{row['retrieved_api_100_oa_percentage_achieve_100']}% (n={row['retrieved_api_100_oa_sum']})\"\n",
        "    percentage_num_retrieved_api_ss = f\"{row['retrieved_api_100_ss_percentage_achieve_100']}% (n={row['retrieved_api_100_ss_sum']})\"\n",
        "    return f\"| {row['source_database']} | {percentage_num_retrieved_id} | {percentage_num_retrieved_id} | {percentage_num_retrieved_api_ss} |\"\n",
        "\n",
        "md_table = \"| Source Database| % of Reviews with 100% Retrievable IDs | % Of Reviews with 100% Retrievable Recall (OpenAlex) | % Of Reviews with 100% Retrievable Recall (Semantic Scholar) | \\n\"\n",
        "md_table += \"|---------------------------------|------------------------------------------------------|---------------------------------------------------------------------|---------------------------------------------------------------------|\\n\"\n",
        "\n",
        "for idx, row in original_sr_performance_df_summary.iterrows():\n",
        "    md_table += format_row(row) + '\\n'\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be86e9ec",
      "metadata": {},
      "source": [
        "# Table 8: Median % (IQR) of included articles with Valid IDs extracted from systematic reviews in dataset, and baseline retrievability rate of included articles across both APIs: (OpenAlex, Semantic Scholar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-theoretical_max_recall_summary",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Source Database| Median % (IQR) of Included Articles with Retrievable IDs | Median (IQR) Maximum Theoretical Recall (OpenAlex) | Median (IQR) Maximum Theoretical Recall (Semantic Scholar) | \n",
              "|---------------------------------|------------------------------------------------------|---------------------------------------------------------------------|---------------------------------------------------------------------|\n",
              "| All Reviews | 86.4 (12.05) | 85.7 (13.2) | 85.7 (16.4) |\n",
              "| CEE | 84.4 (15.52) | 81.6 (15.52) | 83.35 (17.65) |\n",
              "| Campbell | 89.1 (6.3) | 86.4 (4.6) | 87.0 (4.6) |\n",
              "| Cochrane | 85.9 (15.2) | 85.9 (15.2) | 84.0 (18.23) |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#| label: tbl-theoretical_max_recall_summary\n",
        "#| tbl-cap: 'Median (IQR) Theoretical Maximum Achievable Recall by Automated Citation Searching for each Systematic Review Source Database, by API (OpenAlex / Semantic Scholar)'\n",
        "\n",
        "\n",
        " \n",
        "def format_row(row): \n",
        "    median_iqr_retrievable_id = f\"{row['retrievable_id_percentage_oa_median']} ({row['retrievable_id_percentage_oa_iqr']})\"\n",
        "    median_iqr_api_success_retrieved_oa = f\"{row['api_success_retrieved_percentage_oa_median']} ({row['api_success_retrieved_percentage_oa_iqr']})\"\n",
        "    median_iqr_api_success_retrieved_ss = f\"{row['api_success_retrieved_percentage_ss_median']} ({row['api_success_retrieved_percentage_ss_iqr']})\"\n",
        "    return f\"| {row['source_database']} | {median_iqr_retrievable_id} | {median_iqr_api_success_retrieved_oa} | {median_iqr_api_success_retrieved_ss} |\"\n",
        "\n",
        "md_table = \"| Source Database| Median % (IQR) of Included Articles with Retrievable IDs | Median (IQR) Maximum Theoretical Recall (OpenAlex) | Median (IQR) Maximum Theoretical Recall (Semantic Scholar) | \\n\"\n",
        "md_table += \"|---------------------------------|------------------------------------------------------|---------------------------------------------------------------------|---------------------------------------------------------------------|\\n\"\n",
        "\n",
        "for idx, row in original_sr_performance_df_summary.iterrows():\n",
        "    md_table += format_row(row) + '\\n'\n",
        "\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87fcc464",
      "metadata": {},
      "source": [
        "# Table 5 : Median (IQR) Precision, F1 score, F2 score and F3 score for all Search Strategies employed by the Systematic Reviews in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-reference_search_strat_perf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "|Source Database|Precision % (IQR)|F1 Score (IQR)|F2 Score (IQR)|F3 Score (IQR)|\n",
              "|-----------------|---------------|--------------|--------------|--------------|\n",
              "| CEE | 1.15 (3.219) | 0.02 (0.062) | 0.05 (0.138) | 0.10 (0.231) |\n",
              "| Campbell | 0.46 (0.374) | 0.01 (0.007) | 0.02 (0.018) | 0.04 (0.035) |\n",
              "| Cochrane | 3.81 (9.408) | 0.07 (0.168) | 0.16 (0.318) | 0.27 (0.445) |\n",
              "| All Reviews | 0.83 (3.269) | 0.02 (0.063) | 0.04 (0.14) | 0.08 (0.236) |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#| label: tbl-reference_search_strat_perf\n",
        "#| tbl-cap: 'Median (IQR) Precision, F1 score, F2 score and F3 score for all Search Strategies employed by the Systematic Reviews in the Dataset'\n",
        "# Function to calculate median and IQR\n",
        "\n",
        "def iqr(x): \n",
        "    return round(x.quantile(0.75) - x.quantile(0.25),3)\n",
        "baseline_review_data_oa['precision_percentage'] = baseline_review_data_oa['precision']*100\n",
        "relevant_col = ['source_database', 'precision_percentage', 'f1_score', 'f2_score', 'f3_score']\n",
        "\n",
        "baseline_performance_summary_df = baseline_review_data_oa[relevant_col].groupby('source_database').agg({\n",
        "    'precision_percentage': ['median', iqr], 'f1_score': ['median', iqr], 'f2_score': ['median', iqr], 'f3_score': ['median', iqr]\n",
        "    }).reset_index()\n",
        "\n",
        "baseline_performance_summary_df.columns = ['_'.join(col).rstrip('_') for col in baseline_performance_summary_df.columns.values]\n",
        "\n",
        "all_review_row = {\n",
        "    'source_database': 'All Reviews',\n",
        "    'precision_percentage_median': baseline_review_data_oa['precision_percentage'].median(),\n",
        "    'precision_percentage_iqr': iqr(baseline_review_data_oa['precision_percentage']),\n",
        "    'f1_score_median': baseline_review_data_oa['f1_score'].median(),\n",
        "    'f1_score_iqr': iqr(baseline_review_data_oa['f1_score']),\n",
        "    'f2_score_median': baseline_review_data_oa['f2_score'].median(),\n",
        "    'f2_score_iqr': iqr(baseline_review_data_oa['f2_score']),\n",
        "    'f3_score_median': baseline_review_data_oa['f3_score'].median(),\n",
        "    'f3_score_iqr': iqr(baseline_review_data_oa['f3_score'])\n",
        "}\n",
        "\n",
        "baseline_performance_summary_df = pd.concat([baseline_performance_summary_df, pd.DataFrame([all_review_row])], ignore_index=True)\n",
        "\n",
        "def format_row(row): \n",
        "    median_iqr_precision = f\"{row['precision_percentage_median']:.2f} ({row['precision_percentage_iqr']})\"\n",
        "    median_iqr_f1_score = f\"{row['f1_score_median']:.2f} ({row['f1_score_iqr']})\"\n",
        "    median_iqr_f2_score = f\"{row['f2_score_median']:.2f} ({row['f2_score_iqr']})\"\n",
        "    median_iqr_f3_score = f\"{row['f3_score_median']:.2f} ({row['f3_score_iqr']})\"\n",
        "    return f\"| {row['source_database']} | {median_iqr_precision} | {median_iqr_f1_score} | {median_iqr_f2_score} | {median_iqr_f3_score} |\"\n",
        "\n",
        "# Create Markdown table headers\n",
        "md_table = \"|Source Database|Precision % (IQR)|F1 Score (IQR)|F2 Score (IQR)|F3 Score (IQR)|\\n\"\n",
        "md_table += \"|-----------------|---------------|--------------|--------------|--------------|\\n\"\n",
        "for idx, row in baseline_performance_summary_df.iterrows():\n",
        "    md_table += format_row(row) + '\\n'\n",
        "\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b83b5e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\n",
              "|--------|-------------------|----------------------------|----------------------------|\n",
              "| precision_percentage | 9.14 | 0.010* |0.010 *|\n",
              "| f1_score | 9.14 | 0.010* |0.010 *|\n",
              "| f2_score | 9.14 | 0.010* |0.010 *|\n",
              "| f3_score | 9.14 | 0.010* |0.010 *|"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# do some krusksa wallis analysis to see if there is a difference in the performance of the search strategies between the databases\n",
        "from scipy.stats import kruskal, mannwhitneyu\n",
        "from itertools import combinations\n",
        "# Number of Kruskal-Wallis tests being run\n",
        "num_kw_tests = 1  # Only one test per metric\n",
        "\n",
        "# Initialize Markdown table rows for Kruskal-Wallis and Mann-Whitney U\n",
        "markdown_table_rows_kw = [\"| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\",\n",
        "                          \"|--------|-------------------|----------------------------|----------------------------|\"]\n",
        "\n",
        "markdown_table_rows_mwu = [\"| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value |\",\n",
        "                           \"|--------|------------|-------------|-------------|------------------|\"]\n",
        "\n",
        "\n",
        "# New alpha after Bonferroni correction for Kruskal-Wallis\n",
        "alpha_kw = 0.05 / num_kw_tests\n",
        "\n",
        "# Create a list of metrics to test\n",
        "test_df = baseline_review_data_oa.copy()\n",
        "metric_list = ['precision_percentage', 'f1_score', 'f2_score', 'f3_score']\n",
        "\n",
        "for metric in metric_list: \n",
        "    groups = {name: group[metric].dropna().values for name, group in test_df.groupby('source_database')}\n",
        "    if all(len(g) > 1 for g in groups.values()):\n",
        "        k_statistic, k_p_val = kruskal(*groups.values())\n",
        "        adjusted_p_val_kw = min(k_p_val * num_kw_tests, 1)  # Bonferroni correction\n",
        "\n",
        "        asterisk_kw_adj = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        asterisk_kw_raw = \"*\" if k_p_val < 0.05 else \"\"\n",
        "        markdown_table_rows_kw.append(f\"| {metric} | {k_statistic:.2f} | {k_p_val:.3f}{asterisk_kw_raw} |{adjusted_p_val_kw:.3f} {asterisk_kw_adj}|\")\n",
        "        \n",
        "        if k_p_val < alpha_kw:\n",
        "            \n",
        "            mwu_results = []\n",
        "            combos = list(combinations(groups.keys(), 2))  # Convert to list to find length\n",
        "            num_combos = len(combos)  # Number of combinations\n",
        "            for grp1, grp2 in combos:\n",
        "                u_statistic, u_p_val = mannwhitneyu(groups[grp1], groups[grp2])\n",
        "                adjusted_u_p_val = min(u_p_val * num_combos, 1)  # Bonferroni correction\n",
        "               \n",
        "                asterisk_mwu_raw = \"*\" if u_p_val < 0.05 else \"\"\n",
        "                asterisk_mwu_adj = \"*\" if adjusted_u_p_val < 0.05 else \"\"\n",
        "                \n",
        "                markdown_table_rows_mwu.append(f\"| {metric} | {grp1} vs {grp2} | {u_statistic} | {u_p_val:.3f}{asterisk_mwu_raw} | {adjusted_u_p_val:.3f}{asterisk_mwu_adj} |\")\n",
        "\n",
        "# Convert lists of Markdown table rows to single strings\n",
        "markdown_table_kw_studyarea = \"\\n\".join(markdown_table_rows_kw)\n",
        "markdown_table_mwu_studyarea = \"\\n\".join(markdown_table_rows_mwu)\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b6b1be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\n",
              "|--------|-------------------|----------------------------|----------------------------|\n",
              "| precision_percentage | 9.14 | 0.010* |0.010 *|\n",
              "| f1_score | 9.14 | 0.010* |0.010 *|\n",
              "| f2_score | 9.14 | 0.010* |0.010 *|\n",
              "| f3_score | 9.14 | 0.010* |0.010 *|"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(markdown_table_kw_studyarea)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a0ad6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value |\n",
              "|--------|------------|-------------|-------------|------------------|\n",
              "| precision_percentage | CEE vs Campbell | 71.0 | 0.037* | 0.112 |\n",
              "| precision_percentage | CEE vs Cochrane | 24.0 | 0.173 | 0.518 |\n",
              "| precision_percentage | Campbell vs Cochrane | 8.0 | 0.006* | 0.017* |\n",
              "| f1_score | CEE vs Campbell | 71.0 | 0.037* | 0.112 |\n",
              "| f1_score | CEE vs Cochrane | 24.0 | 0.173 | 0.518 |\n",
              "| f1_score | Campbell vs Cochrane | 8.0 | 0.006* | 0.017* |\n",
              "| f2_score | CEE vs Campbell | 71.0 | 0.037* | 0.112 |\n",
              "| f2_score | CEE vs Cochrane | 24.0 | 0.173 | 0.518 |\n",
              "| f2_score | Campbell vs Cochrane | 8.0 | 0.006* | 0.017* |\n",
              "| f3_score | CEE vs Campbell | 71.0 | 0.037* | 0.112 |\n",
              "| f3_score | CEE vs Cochrane | 24.0 | 0.173 | 0.518 |\n",
              "| f3_score | Campbell vs Cochrane | 8.0 | 0.006* | 0.017* |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(markdown_table_mwu_studyarea)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7049ec7f",
      "metadata": {},
      "source": [
        "## Seed Article Candidate Characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8bb293",
      "metadata": {},
      "outputs": [],
      "source": [
        "# seed_article_data\n",
        "\n",
        "seed_articles_campbell_oa = pd.read_excel(excel_path_campbell_oa , sheet_name='sys_rev_seed_candidates_oa')\n",
        "seed_articles_cee_oa = pd.read_excel(excel_path_cee_oa , sheet_name='sys_rev_seed_candidates_oa')\n",
        "seed_articles_cochrane_oa = pd.read_excel(excel_path_cochrane_oa , sheet_name='sys_rev_seed_candidates_oa')\n",
        "\n",
        "seed_article_campbell_ss = pd.read_excel(excel_path_campbell_ss , sheet_name='sys_rev_seed_candidates_ss')\n",
        "seed_article_cee_ss = pd.read_excel(excel_path_cee_ss , sheet_name='sys_rev_seed_candidates_ss')\n",
        "seed_article_cochrane_ss = pd.read_excel(excel_path_cochrane_ss , sheet_name='sys_rev_seed_candidates_ss')\n",
        "\n",
        "seed_articles_cee_oa = seed_articles_cee_oa.query('not no_data_from_api.isna()')\n",
        "seed_articles_campbell_oa = seed_articles_campbell_oa.query('not no_data_from_api.isna()')\n",
        "seed_articles_cochrane_oa = seed_articles_cochrane_oa.query('not no_data_from_api.isna()')\n",
        "seed_articles_all_oa = pd.concat([seed_articles_cee_oa, seed_articles_campbell_oa, seed_articles_cochrane_oa])\n",
        "seed_articles_all_oa \n",
        "\n",
        "seed_article_cee_ss = seed_article_cee_ss.query('not no_data_from_api.isna()')\n",
        "seed_article_campbell_ss = seed_article_campbell_ss.query('not no_data_from_api.isna()')\n",
        "seed_article_cochrane_ss = seed_article_cochrane_ss.query('not no_data_from_api.isna()')\n",
        "seed_articles_all_ss = pd.concat([seed_article_cee_ss, seed_article_campbell_ss, seed_article_cochrane_ss])\n",
        "seed_articles_all_oa.drop(columns = ['originating_api_path'], inplace=True)\n",
        "seed_articles_all_oa = pd.merge(seed_articles_all_oa, baseline_review_data_oa[['id','source_database']], left_on = 'original_sr_id', right_on = 'id', how = 'left')\n",
        "seed_articles_all_ss = pd.merge(seed_articles_all_ss, baseline_review_data_oa[['id','source_database']], left_on = 'original_sr_id', right_on = 'id', how = 'left')\n",
        "seed_articles_all_oa['citation_network_size'] = seed_articles_all_oa['references'] + seed_articles_all_oa['citations']\n",
        "seed_articles_all_ss['citation_network_size'] = seed_articles_all_ss['references'] + seed_articles_all_ss['citations']\n",
        "\n",
        "def percentage_retrieved(x): \n",
        "    return round(100-(sum(x) / len(x) * 100),2)\n",
        "\n",
        "def count_zero(x): \n",
        "    return x.value_counts()[0]\n",
        "\n",
        "\n",
        "seed_articles_all_oa['count'] = 1\n",
        "seed_article_summary_oa = seed_articles_all_oa.groupby(['original_sr_id']).agg({'count' : 'sum', 'no_data_from_api' : [percentage_retrieved,count_zero]}).reset_index()\n",
        "seed_articles_all_ss['count'] = 1\n",
        "seed_article_summary_ss = seed_articles_all_ss.groupby(['original_sr_id']).agg({'count' : 'sum', 'no_data_from_api' : [percentage_retrieved,count_zero]}).reset_index()\n",
        "seed_article_summary_oa\n",
        "\n",
        "#flatten columns \n",
        "seed_article_summary_oa.columns = [\n",
        "    '_'.join(col).rstrip('_') \n",
        "    for col in seed_article_summary_ss.columns.values\n",
        "]\n",
        "\n",
        "seed_article_summary_ss.columns = [\n",
        "    '_'.join(col).rstrip('_') \n",
        "    for col in seed_article_summary_ss.columns.values\n",
        "]\n",
        "\n",
        "seed_article_summary = pd.merge(seed_article_summary_oa, seed_article_summary_ss, on = ['original_sr_id','count_sum'], suffixes = ('_oa', '_ss'))\n",
        "seed_article_summary = pd.merge(seed_article_summary, baseline_review_data_oa[['id','source_database']], left_on = 'original_sr_id', right_on = 'id', how = 'left')\n",
        "seed_article_summary.drop(columns = ['id'], inplace=True)\n",
        "seed_article_summary\n",
        "\n",
        "#col_order \n",
        "col_order = ['source_database', 'original_sr_id', 'count_sum', 'no_data_from_api_percentage_retrieved_oa', 'no_data_from_api_count_zero_oa', 'no_data_from_api_percentage_retrieved_ss','no_data_from_api_count_zero_ss']\n",
        "seed_article_summary = seed_article_summary[col_order]\n",
        "seed_article_summary.rename(columns = {\n",
        "    'source_database':'Source Database',\n",
        "    'count_sum':'Seed Article Candidates Extracted',\n",
        "    'no_data_from_api_percentage_retrieved_oa' : 'Percentage Retrieved (%) - OpenAlex', \n",
        "    'no_data_from_api_percentage_retrieved_ss' : 'Percentage Retrieved (%) - Semantic Scholar', \n",
        "    'no_data_from_api_count_zero_oa' : 'Successful Retrieval - OpenAlex (n)',\n",
        "    'no_data_from_api_count_zero_ss' : 'Successful Retrieval - Semantic Scholar (n)',\n",
        "},\n",
        "    inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5b8c9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_article_summary\n",
        "\n",
        "#calculate the median and IQR for number of seed articles extracted for each source database, and all reviews\n",
        "\n",
        "seed_extract_summary = seed_article_summary.groupby(['Source Database']).agg({'Seed Article Candidates Extracted': ['median', iqr]}).reset_index()\n",
        "seed_extract_summary.columns = ['_'.join(col).rstrip('_') for col in seed_extract_summary.columns.values]\n",
        "\n",
        "#new row for all reviews\n",
        "new_row_data = {\n",
        "    'Source Database': 'All Reviews',\n",
        "    'Seed Article Candidates Extracted_median': seed_article_summary['Seed Article Candidates Extracted'].median(),\n",
        "    'Seed Article Candidates Extracted_iqr': iqr(seed_article_summary['Seed Article Candidates Extracted']),\n",
        "}\n",
        "\n",
        "#append \n",
        "seed_extract_summary = pd.concat([seed_extract_summary, pd.DataFrame([new_row_data])], ignore_index=True)\n",
        "\n",
        "\n",
        "# #| label: tbl-included_articles\n",
        "# #| tbl-cap: 'Median number of included articles (IQR) and average intracluster semantic similarity (+/- SD) for Systematic Reviews in each Source Database, and All Reviews in the Dataset'\n",
        "\n",
        "# original_sr_included_merge = pd.merge(included_all_oa, baseline_review_data_oa, how='left', left_on='original_sr_id', right_on = 'id', suffixes=('_included', '_original_sr'))\n",
        "# # Define IQR function\n",
        "# def iqr(x):\n",
        "#     return x.quantile(0.75) - x.quantile(0.25)\n",
        "\n",
        "# #### ADD IN REFERENCES AND CITATION COUNTS (LATER)\n",
        "    \n",
        "# # Initial grouping and count\n",
        "# col_interest = ['source_database', 'original_sr_id', 'intra_cluster_similarity', 'consolidated_ids']\n",
        "# col_groupby = ['source_database', 'original_sr_id', 'intra_cluster_similarity']\n",
        "# included_summary = original_sr_included_merge[col_interest].groupby(col_groupby).agg({'consolidated_ids': 'count'}).reset_index()\n",
        "# included_summary.rename(columns={'consolidated_ids': 'included_count'}, inplace=True)\n",
        "\n",
        "# # Second-level grouping with median, IQR, mean, and std calculations\n",
        "# included_summary_database = included_summary.groupby(['source_database']).agg({'included_count': ['median', iqr], 'intra_cluster_similarity': ['median', iqr]}).reset_index()\n",
        "\n",
        "# # Flatten the MultiIndex\n",
        "# included_summary_database.columns = ['_'.join(col).rstrip('_') for col in included_summary_database.columns.values]\n",
        "\n",
        "# # Create a summary row based on the included_summary DataFrame\n",
        "# new_row_data = {\n",
        "#     'source_database': 'All Reviews',\n",
        "#     'included_count_median': included_summary['included_count'].median(),\n",
        "#     'included_count_iqr': iqr(included_summary['included_count']),\n",
        "#     'intra_cluster_similarity_median': included_summary['intra_cluster_similarity'].median(),\n",
        "#     'intra_cluster_similarity_iqr': iqr(included_summary['intra_cluster_similarity'])\n",
        "# }\n",
        "\n",
        "# # Append the new summary row\n",
        "# included_summary_database = pd.concat([included_summary_database, pd.DataFrame([new_row_data])], ignore_index=True)\n",
        "\n",
        "# # Function to format rows for the Markdown table\n",
        "# def format_row(row):\n",
        "#     median_iqr_count = f\"{row['included_count_median']} ({row['included_count_iqr']})\"\n",
        "#     median_iqr_intrasim= f\"{row['intra_cluster_similarity_median']:.3f} ({row['intra_cluster_similarity_iqr']:.3f})\"\n",
        "#     return f\"| {row['source_database']} | {median_iqr_count} | {median_iqr_intrasim} |\"\n",
        "\n",
        "# # Create Markdown table headers\n",
        "# md_table = \"|Source Database|Number of Included Articles (IQR)|Intra-Cluster Similarity (IQR)|\\n\"\n",
        "# md_table += \"|-----------------|---------------------------------|---------------------------------------------|\\n\"\n",
        "\n",
        "# # Add rows to the Markdown table\n",
        "# for idx, row in included_summary_database.iterrows():\n",
        "#     md_table += format_row(row) + '\\n'\n",
        "\n",
        "seed_extract_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede5ca5b",
      "metadata": {},
      "source": [
        "# Table 4 : Summary Baseline Characteristics of Seed Articles Successfully Retrieved from the OpenAlex and Semantic Scholar APIs (OpenAlex)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-seed_baseline_char_oa",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-seed_baseline_char_oa\n",
        "#| tbl-cap: Summary Baseline Characteristics of Seed Article Candidates Sucessfully Retrieved from the OpenAlex API\n",
        "\n",
        "seed_article_all_oa_retrieved = seed_articles_all_oa.query('no_data_from_api == 0').copy()\n",
        "seed_article_all_oa_retrieved['article_type'] = seed_article_all_oa_retrieved['article_type'].str.capitalize()\n",
        "seed_article_all_oa_retrieved_allarticles = seed_article_all_oa_retrieved.copy()\n",
        "seed_article_all_oa_retrieved_allarticles['article_type'] = 'Overall'\n",
        "seed_article_all_oa_retrieved_allarticles = pd.concat([seed_article_all_oa_retrieved_allarticles, seed_article_all_oa_retrieved])\n",
        "article_type_characteristic_summary_oa = seed_article_all_oa_retrieved_allarticles.groupby(['article_type']).agg({\n",
        "    'references' : ['median', iqr],\n",
        "    'citations' : ['median', iqr], \n",
        "    'citation_network_size' : ['median', iqr],\n",
        "    'year' : ['median', iqr],\n",
        "    'article_type' : ['count']\n",
        "    }).reset_index()\n",
        "article_type_characteristic_summary_oa\n",
        "\n",
        "\n",
        "#flatten column multi index \n",
        "article_type_characteristic_summary_oa.columns = ['_'.join(col).strip() for col in article_type_characteristic_summary_oa.columns.values]\n",
        "footnote_dict = {\n",
        "    'Overall' : '*',\n",
        "    'Median Citation Network Size (IQR)' : '**',\n",
        "    'Other' : 'ยง'\n",
        "}\n",
        "# #order rows \n",
        "row_order = ['Overall', 'Research article', 'Evidence synthesis', 'Methodology', 'Commentary', 'Framework', 'Consensus article', 'Other']\n",
        "article_type_characteristic_summary_oa['article_type_'] = pd.Categorical(article_type_characteristic_summary_oa['article_type_'], categories=row_order, ordered=True)\n",
        "article_type_characteristic_summary_oa.sort_values('article_type_', inplace=True)\n",
        "sorting_rows = article_type_characteristic_summary_oa.iloc[1:8].copy()  # Adjust indices based on your specific case\n",
        "sorting_rows.sort_values('citation_network_size_median', ascending = False, inplace=True)\n",
        "article_type_characteristic_summary_oa.iloc[1:8] = sorting_rows\n",
        "\n",
        "\n",
        "def format_row(row, footnote_dict): \n",
        "\n",
        "    median_iqr_year = f\"{row['year_median']} ({row['year_iqr']})\"\n",
        "    median_iqr_references = f\"{row['references_median']} ({row['references_iqr']})\"\n",
        "    median_iqr_citations = f\"{row['citations_median']} ({row['citations_iqr']})\"\n",
        "    median_iqr_network_size = f\"{row['citation_network_size_median']} ({row['citation_network_size_iqr']})\"\n",
        "    if footnote_dict is not None:\n",
        "        footnote = footnote_dict.get(row['article_type_'], '')\n",
        "    return f\"| {row['article_type_']}{footnote} | {row['article_type_count']} | {median_iqr_year} | {median_iqr_references} | {median_iqr_citations} | {median_iqr_network_size} |\"\n",
        "\n",
        "md_table = \"| Article Type | Article Count | Median Year (IQR) | Median References (IQR) | Median Citations (IQR) | Median Citation Network Size (IQR) |\\n\"\n",
        "\n",
        "md_table += \"| --- | --- | --- | --- | --- | --- |\\n\"\n",
        "#defining footnotes\n",
        "md_table_footnote = \"\\n\\* _Aggregate of all article types. Only candidates with retrievable DOIs were extracted and retrieved._\\n\"\n",
        "md_table_footnote += \"\\n\\** _Represents sum of number of references and citations_ \\n\"\n",
        "md_table_footnote  += \"\\n ยง _Grey literature, includes datasets, working papers, reports, etc._ \\n\"\n",
        "\n",
        "#constructing table\n",
        "for idx, row in article_type_characteristic_summary_oa.iterrows():\n",
        "    md_table += format_row(row,footnote_dict) + '\\n'\n",
        "md_table += md_table_footnote\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a69fde0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Table 4 : Summary Baseline Characteristics of Seed Articles Successfully Retrieved from the OpenAlex and Semantic Scholar APIs (Semantic Scholar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-seed_baseline_char_ss",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-seed_baseline_char_ss\n",
        "#| tbl-cap: Summary Baseline Characteristics of Seed Article Candidates Sucessfully Retrieved from the Semantic Scholar API\n",
        "\n",
        "seed_article_all_ss_retrieved = seed_articles_all_ss.query('no_data_from_api == 0').copy()\n",
        "seed_article_all_ss_retrieved['article_type'] = seed_article_all_ss_retrieved['article_type'].str.capitalize()\n",
        "seed_article_all_ss_retrieved_allarticles = seed_article_all_ss_retrieved.copy()\n",
        "seed_article_all_ss_retrieved_allarticles['article_type'] = 'Overall'\n",
        "\n",
        "seed_article_all_ss_retrieved_allarticles = pd.concat([seed_article_all_ss_retrieved_allarticles, seed_article_all_ss_retrieved])\n",
        "article_type_characteristic_summary_ss = seed_article_all_ss_retrieved_allarticles.groupby(['article_type']).agg({\n",
        "    'references' : ['median', iqr],\n",
        "    'citations' : ['median', iqr], \n",
        "    'citation_network_size' : ['median', iqr],\n",
        "    'year' : ['median', iqr],\n",
        "    'article_type' : ['count']\n",
        "    }).reset_index()\n",
        "\n",
        "#flatten column multi index \n",
        "article_type_characteristic_summary_ss.columns = ['_'.join(col).strip() for col in article_type_characteristic_summary_ss.columns.values]\n",
        "\n",
        "row_order = ['Overall', 'Research article', 'Evidence synthesis', 'Methodology', 'Commentary', 'Framework', 'Consensus article', 'Other']\n",
        "# #order rows \n",
        "article_type_characteristic_summary_ss['article_type_'] = pd.Categorical(article_type_characteristic_summary_ss['article_type_'], categories=row_order, ordered=True)\n",
        "article_type_characteristic_summary_ss.sort_values('article_type_', inplace=True)\n",
        "sorting_rows = article_type_characteristic_summary_ss.iloc[1:8].copy()  # Adjust indices based on your specific case\n",
        "sorting_rows.sort_values('citation_network_size_median', ascending = False, inplace=True)\n",
        "article_type_characteristic_summary_ss.iloc[1:8] = sorting_rows\n",
        "\n",
        "\n",
        "footnote_dict = {\n",
        "    'Overall' : '*',\n",
        "    'Median Citation Network Size (IQR)' : '**',\n",
        "    'Other' : 'ยง'\n",
        "}\n",
        "md_table = \"| Article Type | Article Count | Median Year (IQR) | Median References (IQR) | Median Citations (IQR) | Median Citation Network Size (IQR) |\\n\"\n",
        "md_table += \"| --- | --- | --- | --- | --- | --- |\\n\"\n",
        "#defining footnotes\n",
        "md_table_footnote = \"\\n\\* _Aggregate of all article types. Only candidates with retrievable DOIs were extracted and retrieved._\\n\"\n",
        "md_table_footnote += \"\\n\\** _Represents sum of number of references and citations_  \\n\"\n",
        "md_table_footnote  += \"\\n ยง _Grey literature, includes datasets, working papers, reports, etc._ \\n\"\n",
        "\n",
        "#constructing table\n",
        "for idx, row in article_type_characteristic_summary_ss.iterrows():\n",
        "    md_table += format_row(row,footnote_dict) + '\\n'\n",
        "md_table += md_table_footnote\n",
        "Markdown(md_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988fcbb9",
      "metadata": {},
      "source": [
        "## Performance of Automated Citation Searching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a117d0d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#reading in data to construct dataframes for summary tables. However this is also good for supplementary table later. \n",
        "\n",
        "### SUPPLEMENTARY TABLE \n",
        "\n",
        "result_path = '..\\Dataset\\Results\\citation_mining_results\\{}'\n",
        "\n",
        "campbell_results_oa = pd.read_csv(result_path.format('campbell_results_oa.csv'))\n",
        "campbell_results_ss = pd.read_csv(result_path.format('campbell_results_ss.csv'))\n",
        "cee_results_oa = pd.read_csv(result_path.format('cee_results_oa.csv'))\n",
        "cee_results_ss = pd.read_csv(result_path.format('cee_results_ss.csv'))\n",
        "cochrane_results_ss = pd.read_csv(result_path.format('cochrane_results_ss.csv'))\n",
        "cochrane_results_oa = pd.read_csv(result_path.format('cochrane_results_oa.csv'))\n",
        "full_results_oa = pd.concat([campbell_results_oa, cee_results_oa, cochrane_results_oa], ignore_index=True)\n",
        "full_results_ss = pd.concat([campbell_results_ss, cee_results_ss, cochrane_results_ss], ignore_index=True)\n",
        "\n",
        "def convert_to_list(entry):\n",
        "    try:\n",
        "        # Try to evaluate string as list\n",
        "        evaluated_entry = ast.literal_eval(entry)\n",
        "        if isinstance(evaluated_entry, list):\n",
        "            return evaluated_entry\n",
        "    except (ValueError, SyntaxError):\n",
        "        # If evaluation fails, it's a simple string\n",
        "        pass\n",
        "    # If it's not a list, wrap it in a list\n",
        "    return [entry]\n",
        "\n",
        "def string_to_list(x):\n",
        "    if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
        "        return ast.literal_eval(x)\n",
        "    return x\n",
        "\n",
        "# columns of interest - colums starting with metrics. and params. \n",
        "\n",
        "metrics_cols = [col for col in full_results_oa.columns if col.startswith('metrics.')]\n",
        "params_cols = [col for col in full_results_oa.columns if col.startswith('params.')]\n",
        "\n",
        "\n",
        "full_results_oa['params.seed_num'] = full_results_oa['params.seed_id'].apply(convert_to_list).apply(len)\n",
        "full_results_ss['params.seed_num'] = full_results_ss['params.seed_id'].apply(convert_to_list).apply(len)\n",
        "full_results = pd.concat([full_results_oa, full_results_ss], ignore_index=True)\n",
        "full_results_valid_recall = full_results.query('`metrics.recall` > 0')\n",
        "\n",
        "full_results_valid_recall.rename(columns = {\n",
        "    'metrics.recall': 'recall_auto',\n",
        "    'metrics.precision': 'precision_auto',\n",
        "    'metrics.f1_score': 'f1_score_auto',\n",
        "    'metrics.f2_score': 'f2_score_auto',\n",
        "    'metrics.f3_score': 'f3_score_auto',\n",
        "    'params.original_review_id': 'Original Systematic Review ID',\n",
        "    'params.api': 'API',\n",
        "}, inplace=True)\n",
        "\n",
        "\n",
        "#extract best performing run for each original review id \n",
        "result_best_performing = (\n",
        "    full_results_valid_recall\n",
        "    .sort_values(by=['Original Systematic Review ID', 'recall_auto', 'f3_score_auto'], ascending=[True, False, False])\n",
        "    .groupby('Original Systematic Review ID')\n",
        "    .first()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "result_recall_sorted_extract = result_best_performing.copy()\n",
        "result_recall_sorted_extract = pd.merge(result_recall_sorted_extract, baseline_review_data_oa[['id', 'source_database']], how='left', left_on='Original Systematic Review ID', right_on='id')\n",
        "result_recall_sorted_extract.rename(columns = {'source_database' : 'Reference Systematic Review Source Database'}, inplace = True)\n",
        "metrics = ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']\n",
        "\n",
        "# Data Preparation\n",
        "metrics = ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']\n",
        "col_interest = ['Original Systematic Review ID','Reference Systematic Review Source Database', 'API', 'params.seed_num', 'params.seed_id', 'run_id'] + metrics\n",
        "\n",
        "result_summary = result_recall_sorted_extract[col_interest].copy()\n",
        "\n",
        "original_sr_performance_df_summary_comparison = original_sr_performance_df_summary.query('source_database != \"All Reviews\"').copy()\n",
        "original_sr_performance_df_summary_comparison[['api_success_retrieved_percentage_oa_median', 'api_success_retrieved_percentage_ss_median']]\n",
        "baseline_review_result_extract = baseline_review_data_oa[['id', 'source_database', 'recall','precision', 'f1_score', 'f2_score', 'f3_score']].copy()\n",
        "baseline_review_result_extract.rename(columns = {'recall' : 'recall_sr', 'precision' : 'precision_sr', 'f1_score' : 'f1_score_sr', 'f2_score' : 'f2_score_sr', 'f3_score' : 'f3_score_sr'}, inplace = True)\n",
        "result_summary = result_summary.merge(baseline_review_result_extract, left_on = ['Original Systematic Review ID','Reference Systematic Review Source Database'], right_on = ['id','source_database'], how = 'left')\n",
        "#drop redundant columns\n",
        "result_summary.drop(columns = ['id', 'source_database'], inplace = True)\n",
        "\n",
        "## merging with maximum theoretical recall \n",
        "extract_col = ['id', 'source_database', 'included_num','intra_cluster_similarity','included_study_type_selection_criteria','api_success_retrieved_percentage_ss', 'api_success_retrieved_percentage_oa']\n",
        "original_sr_all_melt = pd.melt(original_sr_all[extract_col], \n",
        "                               id_vars=['id', 'source_database','included_num','intra_cluster_similarity','included_study_type_selection_criteria'], \n",
        "                               value_vars = ['api_success_retrieved_percentage_ss', 'api_success_retrieved_percentage_oa'],\n",
        "                               var_name = 'API', \n",
        "                               value_name = 'Maximum Theoretical Recall (%)'\n",
        "                               )\n",
        "original_sr_all_melt.rename(columns = {'source_database' : 'Reference Systematic Review Source Database', 'id' : 'Original Systematic Review ID'}, inplace = True)\n",
        "original_sr_all_melt['API'] = original_sr_all_melt['API'].apply(lambda x: 'semanticscholar' if x.endswith('_ss') else 'openalex')\n",
        "result_summary_maxrecall = pd.merge(result_summary, original_sr_all_melt, left_on = ['Original Systematic Review ID','Reference Systematic Review Source Database', 'API'], right_on = ['Original Systematic Review ID','Reference Systematic Review Source Database', 'API'], how = 'inner')\n",
        "result_summary_maxrecall['recall_auto_percentage'] = result_summary_maxrecall['recall_auto'] * 100\n",
        "result_summary_maxrecall['recall_sr_percentage'] = result_summary_maxrecall['recall_sr'] * 100\n",
        "result_summary_maxrecall['precision_auto_percentage'] = result_summary_maxrecall['precision_auto'] * 100\n",
        "result_summary_maxrecall['precision_sr_percentage'] = result_summary_maxrecall['precision_sr'] * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b69660",
      "metadata": {},
      "outputs": [],
      "source": [
        "result_summary_maxrecall.rename(columns = {'Maximum Theoretical Recall (%)' : 'Baseline Retrievability Rate (%)'}, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebdb0b93",
      "metadata": {},
      "source": [
        "# Figure 4: Recall of automated citation searching for each systematic review against various level of recall (A), and against the baseline retrievability rate of included articles of each systematic review (B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-recalltheoretical",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-recalltheoretical\n",
        "#| fig-cap: 'Recall of automated citation searching for each systematic review against various level of recall (A), and against theoretical maximum recall of each systematic review (B)'\n",
        "import math\n",
        "# Function to Add Dotted Lines and Annotations\n",
        "def add_dotted_lines(ax, y_values, colors, labels, y_offset):\n",
        "    for y, color, label in zip(y_values, colors, labels):\n",
        "        ax.axhline(y=y, color=color, linestyle='--')\n",
        "        ax.text(0.5, y - y_offset, f\"{int(y)}% Recall\", color=color, ha=\"center\", va=\"center\")\n",
        "\n",
        "def add_line_annotations(ax, x_start, x_end, y_start, y_end, labels, x_offset, y_offset):\n",
        "    for xs, xe, ys, ye, label in zip(x_start, x_end, y_start, y_end, labels):\n",
        "        delta_x = xe - xs\n",
        "        delta_y = ye - ys\n",
        "        angle_offset = -5\n",
        "        angle = math.atan2(delta_y, delta_x) * (180.0 / math.pi)\n",
        "        xm = (xs + xe) / 2\n",
        "        ym = (ys + ye) / 2 + y_offset\n",
        "        ax.text(xm, ym, label, rotation = angle + angle_offset, ha='center', va='center', fontsize=8)\n",
        "\n",
        "\n",
        "# Initialize y_offset and other variables for dotted lines\n",
        "y_values = [100, 80, 50]\n",
        "colors = ['g', 'y', 'r']\n",
        "labels = ['100% Recall', '80% Recall', '50% Recall']\n",
        "\n",
        "# Create the 1x2 Plot grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
        "# Add subplot letters\n",
        "subplot_letters = ['A', 'B', 'C', 'D']\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "# Plot for Recall against various benchmarks (targets)\n",
        "sns.scatterplot(x='Reference Systematic Review Source Database', \n",
        "                y='recall_auto_percentage', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[0])\n",
        "axes[0].legend().remove()\n",
        "add_dotted_lines(axes[0], y_values, colors, labels, y_offset=5)\n",
        "\n",
        "# Plot for Recall against Theoretical MAximum Recall\n",
        "sns.scatterplot(x='Baseline Retrievability Rate (%)',\n",
        "                y='recall_auto_percentage', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[1])\n",
        "axes[1].plot([40, 100], [40,100], color='g', linestyle='--')\n",
        "axes[1].plot([40, 100], [32,80], color='y', linestyle='--')\n",
        "axes[1].plot([40, 100], [20,50], color='r', linestyle='--')\n",
        "axes[1].legend().remove()\n",
        "\n",
        "x_start = [40, 40, 40]\n",
        "x_end = [100, 100, 100]\n",
        "y_start = [40, 32, 20]\n",
        "y_end = [100, 80, 50]\n",
        "labels = [\"Automated Recall = Baseline Retrievability Rate\", \"Automated Recall >= 80% of Baseline Retrievability Rate\", \"Automated Recall is >= 50% of Baseline Retrievability Rate\"]\n",
        "\n",
        "# Add Annotations\n",
        "add_line_annotations(axes[1], x_start, x_end, y_start, y_end, labels, x_offset = 0,  y_offset=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4408b933",
      "metadata": {},
      "source": [
        "# Table 6: Performance (Precision, F1, F2 and F3 Score) of automated citation searching vs reference systematic review search strategies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-perfcomparison_autosr",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-perfcomparison_autosr\n",
        "#| tbl-cap: 'Performance (Precision, F1, F2 and F3 Score) of automated citation searching vs sample systematic review.'\n",
        "\n",
        "from scipy.stats import mannwhitneyu\n",
        "result_summary_maxrecall_comparison = result_summary_maxrecall.copy() \n",
        "result_summary_maxrecall_comparison.rename(columns = {\n",
        "    'recall_auto_percentage' : 'recall_percentage_auto', \n",
        "    'precision_auto_percentage' : 'precision_percentage_auto',\n",
        "    'recall_sr_percentage' : 'recall_percentage_sr', \n",
        "    'precision_sr_percentage' : 'precision_percentage_sr'}, inplace=True)\n",
        "markdown_table_rows_mwu_autosr = [\"| Metric | Automated Citation Searching Performance | Sample Systematic Review Performance | P-value |\",\n",
        "                           \"|--------|----------------|----------------|-------------|\"]\n",
        "\n",
        "\n",
        "# Loop through each metric\n",
        "for metric in [ 'recall_percentage','precision_percentage', 'f1_score', 'f2_score', 'f3_score']:\n",
        "    metric_a = f\"{metric}_auto\"\n",
        "    metric_b = f\"{metric}_sr\"\n",
        "    \n",
        "    group_a = result_summary_maxrecall_comparison[metric_a]\n",
        "    group_b = result_summary_maxrecall_comparison[metric_b]\n",
        "\n",
        "    median_a = np.median(group_a)\n",
        "    median_b = np.median(group_b)\n",
        "    iqr_a = iqr(group_a)\n",
        "    iqr_b = iqr(group_b)\n",
        "    group_a_str = f\"{median_a:.3f} ({iqr_a:.3f})\"\n",
        "    group_b_str = f\"{median_b:.3f} ({iqr_b:.3f})\"\n",
        "    \n",
        "    if len(group_a) > 1 and len(group_b) > 1:\n",
        "        u_statistic, u_p_val = mannwhitneyu(group_a, group_b)\n",
        "        \n",
        "        # Bonferroni correction; here it's trivial as you have only two groups for comparison.\n",
        "        adjusted_u_p_val = min(u_p_val * 1, 1)  \n",
        "\n",
        "        asterisk_mwu_raw = \"*\" if u_p_val < 0.05 else \"\"\n",
        "        asterisk_mwu_adj = \"*\" if adjusted_u_p_val < 0.05 else \"\"\n",
        "\n",
        "        markdown_table_rows_mwu_autosr.append(f\"| {metric} |  {group_a_str} | {group_b_str}| {adjusted_u_p_val:.3f}{asterisk_mwu_adj} |\")\n",
        "\n",
        "# Convert list of Markdown table rows to a single string\n",
        "markdown_table_mwu_autosr = \"\\n\".join(markdown_table_rows_mwu_autosr)\n",
        "Markdown(markdown_table_mwu_autosr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7733c4bc",
      "metadata": {},
      "source": [
        "# Figure 3: (A-D) Comparison of Automated Citation Searching Performance (Best Performing Run) vs Search Strategies employed by Sample Systematic Review, by Precision, F1 Score, F2 Score and F3 score. Dots above the dotted line represents outperformance relative to the reference standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-autovssr",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-autovssr\n",
        "#| fig-cap: '(A-D) Comparison of Automated Citation Searching Performance (Best Performing Run) vs Search Strategies employed by Sample Systematic Review, by Precision, F1 Score, F2 Score and F3 score.'\n",
        "#| fig-pos: p\n",
        "\n",
        "# Function to Add Dotted Lines and Annotations\n",
        "def add_dotted_lines(ax, y_values, colors, labels, y_offset=0.08):\n",
        "    for y, color, label in zip(y_values, colors, labels):\n",
        "        ax.axhline(y=y, color=color, linestyle='--')\n",
        "        ax.text(0.5, y - y_offset, f\"{int(y*100)}% Recall\", color=color, ha=\"center\", va=\"center\")\n",
        "\n",
        "\n",
        "# Initialize y_offset and other variables for dotted lines\n",
        "y_offset_value = 0.02\n",
        "y_values = [1.0, 0.8, 0.5]\n",
        "colors = ['g', 'y', 'r']\n",
        "labels = ['100% Recall', '80% Recall', '50% Recall']\n",
        "\n",
        "# Create the 2x2 Plot grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 15))\n",
        "# Add subplot letters\n",
        "subplot_letters = ['A', 'B', 'C', 'D']\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "\n",
        "# Plot for Precision\n",
        "sns.scatterplot(x='precision_sr',\n",
        "                y='precision_auto', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[0, 0])\n",
        "axes[0, 0].plot([0, 0.5], [0, 0.5], color='black', linestyle='--')\n",
        "# axes[0, 0].annotate(\"Above this line: Automated method  \\n outperforms reference standard (Precision)\", xy=(0.25, 0.25), xytext=(0.05, 0.4),\n",
        "#                  arrowprops=dict(arrowstyle=\"->\"), fontsize=10)\n",
        "axes[0, 0].legend().remove()\n",
        "\n",
        "# Plot for F1 Score\n",
        "sns.scatterplot(x='f1_score_sr',\n",
        "                y='f1_score_auto', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[0, 1])\n",
        "axes[0, 1].legend().remove()\n",
        "axes[0, 1].plot([0, 0.5], [0, 0.5], color='black', linestyle='--')\n",
        "# axes[0, 1].annotate(\"Above this line: Automated method  \\n outperforms reference standard (F1 Score)\", xy=(0.25, 0.25), xytext=(0.05, 0.4),\n",
        "#                  arrowprops=dict(arrowstyle=\"->\"), fontsize=10)\n",
        "axes[0, 1].legend().remove()\n",
        "\n",
        "# Plot for F2 Score\n",
        "sns.scatterplot(x='f2_score_sr',\n",
        "                y='f2_score_auto', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[1, 0])\n",
        "axes[1, 0].legend().remove()\n",
        "axes[1, 0].plot([0, 0.5], [0, 0.5], color='black', linestyle='--')\n",
        "# axes[1, 0].annotate(\"Above this line: Automated method  \\n outperforms reference standard (F2 Score)\", xy=(0.4, 0.4), xytext=(0.3, 0.2),\n",
        "#                  arrowprops=dict(arrowstyle=\"->\"), fontsize=10)\n",
        "axes[1, 0].legend().remove()\n",
        "\n",
        "plt.subplots_adjust(bottom=0.4)\n",
        "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='lower center', ncol=len(labels), title='Systematic Review Database', bbox_to_anchor=(0.5, 0.08))  # Adjusted bbox_to_anchor\n",
        "\n",
        "# Plot for F3 Score\n",
        "sns.scatterplot(x='f3_score_sr',\n",
        "                y='f3_score_auto', \n",
        "                data=result_summary_maxrecall, \n",
        "                hue='Reference Systematic Review Source Database', \n",
        "                ax=axes[1, 1])\n",
        "axes[1, 1].legend().remove()\n",
        "axes[1, 1].plot([0, 0.5], [0, 0.5], color='black', linestyle='--')\n",
        "# axes[1, 1].annotate(\"Above this line: Automated method  \\n outperforms reference standard (F3 Score)\", xy=(0.4, 0.4), xytext=(0.3, 0.2),\n",
        "#                  arrowprops=dict(arrowstyle=\"->\"), fontsize=10)\n",
        "axes[1, 1].legend().remove()\n",
        "\n",
        "plt.subplots_adjust(bottom=0.4)\n",
        "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='lower center', ncol=len(labels), title='Systematic Review Database', bbox_to_anchor=(0.5, 0.08))  # Adjusted bbox_to_anchor\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjusted the tight_layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a0c16c",
      "metadata": {},
      "source": [
        "# Factors Influencing Automated Citation Searching Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-numerical_variable_performance_comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-numerical_variable_performance_comparison\n",
        "#| tbl-cap: 'Spearman rank correlation coefficients (Rho, Adjusted P-value (After Bonferroni Correctionm 5 Comparisons for each outcome metric) for intracluster semantic similiarity, number of included articles, number of seed articles, against outcome metrics (Recall, Precision, F(1-3 Score))'\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "# Assuming result_summary_maxrecall is your DataFrame\n",
        "result_summary_maxrecall.rename(columns = {\n",
        "    'params.seed_num': 'Number of Seed Articles',\n",
        "    'included_num' : 'Number of Included Articles',\n",
        "    'intra_cluster_similarity' : 'Intra-cluster Semantic Similarity'}, inplace = True)\n",
        "\n",
        "# Define separate lists for factors and outcome metrics\n",
        "factors = ['Intra-cluster Semantic Similarity', 'Number of Included Articles', 'Number of Seed Articles']\n",
        "outcome_metrics = ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']\n",
        "\n",
        "# Calculate number of comparisons made: One for each combination of factors and outcome metrics\n",
        "num_comparisons = len(factors)\n",
        "\n",
        "# Initialize DataFrames to store Spearman correlation coefficients and p-values\n",
        "spearman_corr_matrix = pd.DataFrame(index=outcome_metrics, columns=factors)\n",
        "spearman_pvalue_matrix = pd.DataFrame(index=outcome_metrics, columns=factors)\n",
        "spearman_adjusted_pvalue_matrix = pd.DataFrame(index=outcome_metrics, columns=factors)\n",
        "\n",
        "# Calculate Spearman correlations and p-values\n",
        "for factor in factors:\n",
        "    for metric in outcome_metrics:\n",
        "        rho, p_value = spearmanr(result_summary_maxrecall[factor], result_summary_maxrecall[metric])\n",
        "        adjusted_p_value = min(p_value * num_comparisons, 1)\n",
        "        \n",
        "        spearman_corr_matrix.loc[metric, factor] = rho\n",
        "        spearman_pvalue_matrix.loc[metric, factor] = p_value\n",
        "        spearman_adjusted_pvalue_matrix.loc[metric, factor] = adjusted_p_value\n",
        "\n",
        "# Combine rho, raw p-value, and adjusted p-value into a single DataFrame\n",
        "combine_corr_pvalue = pd.DataFrame(index=spearman_corr_matrix.index, columns=spearman_corr_matrix.columns)\n",
        "for row in spearman_corr_matrix.index:\n",
        "    for col in spearman_corr_matrix.columns:\n",
        "        rho = spearman_corr_matrix.loc[row, col]\n",
        "        raw_pval = spearman_pvalue_matrix.loc[row, col]\n",
        "        adj_pval = spearman_adjusted_pvalue_matrix.loc[row, col]\n",
        "        if pd.isna(rho) or pd.isna(raw_pval) or pd.isna(adj_pval):\n",
        "            combine_corr_pvalue.loc[row, col] = None\n",
        "        else:\n",
        "            combine_corr_pvalue.loc[row, col] = f\"Rho={rho:.3f}, Raw_p={raw_pval:.3f}, Adj_p={adj_pval:.3f}\"\n",
        "\n",
        "# Convert DataFrame to Markdown table using tabulate\n",
        "corr_table = tabulate(combine_corr_pvalue, headers='keys', tablefmt='pipe', numalign=\"right\")\n",
        "Markdown(corr_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785c9a6f",
      "metadata": {},
      "source": [
        "### Study Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-automated_citation_search_perf_summary",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-automated_citation_search_perf_summary\n",
        "#| tbl-cap: 'Median (IQR) Recall, Precision, F1 score, F2 score, F3 score of the best performing automated citation searching runs, by Systematic Review Subsets'\n",
        "\n",
        "result_summary_database_source = result_summary.copy()\n",
        "result_summary_database_source_all = result_summary.copy()\n",
        "result_summary_database_source_all['Reference Systematic Review Source Database'] = 'All Reviews'\n",
        "result_summary_database_source = pd.concat([result_summary_database_source, result_summary_database_source_all], ignore_index=True)\n",
        "result_summary_database_source_grp = result_summary_database_source.groupby(['Reference Systematic Review Source Database']).agg(\n",
        "    {\n",
        "        'recall_auto' : ['median', iqr],\n",
        "        'precision_auto' : ['median', iqr],\n",
        "        'f1_score_auto' : ['median', iqr],\n",
        "        'f2_score_auto' : ['median', iqr],\n",
        "        'f3_score_auto' : ['median', iqr],\n",
        "        'recall_sr' : ['median', iqr],\n",
        "        'precision_sr' : ['median', iqr],\n",
        "        'f1_score_sr' : ['median', iqr],\n",
        "        'f2_score_sr' : ['median', iqr],\n",
        "        'f3_score_sr' : ['median', iqr],\n",
        "    }\n",
        ").reset_index()\n",
        "metrics_auto = ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']\n",
        "result_summary_database_source_grp.columns = [\n",
        "    '_'.join(col).rstrip('_') \n",
        "    for col in result_summary_database_source_grp.columns.values\n",
        "]\n",
        "\n",
        "footnote_dict = {\n",
        "    'All Reviews' : '*'\n",
        "}\n",
        "\n",
        "\n",
        "def format_row(row,footnote_dict):\n",
        "    median_iqr_recall = f\"{row['recall_auto_median']*100:.2f} ({row['recall_auto_iqr']*100:.2f})\"\n",
        "    median_iqr_precision = f\"{row['precision_auto_median']*100:.2f} ({row['precision_auto_iqr']*100:.2f})\"\n",
        "    median_iqr_f1_score = f\"{row['f1_score_auto_median']:.3f} ({row['f1_score_auto_iqr']:.3f})\"\n",
        "    median_iqr_f2_score = f\"{row['f2_score_auto_median']:.3f} ({row['f2_score_auto_iqr']:.3f})\"\n",
        "    median_iqr_f3_score = f\"{row['f3_score_auto_median']:.3f} ({row['f3_score_auto_iqr']:.3f})\"\n",
        "    if footnote_dict is not None: \n",
        "        footnote = footnote_dict.get(row['Reference Systematic Review Source Database'], '')\n",
        "    return f\"| {row['Reference Systematic Review Source Database']}{footnote} | {median_iqr_recall} | {median_iqr_precision} | {median_iqr_f1_score} | {median_iqr_f2_score} | {median_iqr_f3_score} |\"\n",
        "\n",
        "md_table = \"| Systematic Review Subset | Median (IQR) Recall (%) | Median (IQR) Precision (%) | Median (IQR) F1 Score | Median (IQR) F2 Score | Median (IQR) F3 Score |\\n\"\n",
        "md_table += \"|---|---|---|---|---|---|\\n\"\n",
        "\n",
        "#define footnotes\n",
        "md_table_footnotes = \"\\n \\* Aggregate of all systematic reviews\"\n",
        "\n",
        "for idx, row in result_summary_database_source_grp.iterrows():\n",
        "    md_table += format_row(row, footnote_dict) + '\\n'\n",
        "md_table += md_table_footnotes\n",
        "md_table_srsubsets_autoperf  = md_table\n",
        "Markdown(md_table_srsubsets_autoperf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22340ade",
      "metadata": {},
      "source": [
        "# Supplementary Fig: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-srsubset",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-srsubset\n",
        "#| fig-cap: '(A-D) Comparison of Automated Citation Searching Performance (Best Performing Run) within different Systematic Review Subsets, by Precision, F1 Score, F2 Score and F3 score.'\n",
        "#| fig-pos: p\n",
        "\n",
        "# Create the 2x2 Plot grid\n",
        "from matplotlib.patches import Patch\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 15))\n",
        "\n",
        "# Subplot letters and custom color palette\n",
        "subplot_letters = ['A', 'B', 'C', 'D']\n",
        "metrics = ['precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']\n",
        "custom_palette = {'Cochrane': '#809fff',  # Light blue\n",
        "                  'CEE': '#70db70',       \n",
        "                  'Campbell': '#ffd480'}\n",
        "\n",
        "def add_significance_bracket(ax, x1, x2, y_max, y_range):\n",
        "    y = y_max + 0.05 * y_range\n",
        "    bracket_height = 0.02 * y_range\n",
        "    text_y = y_max + 0.065 * y_range\n",
        "    p_value = 0.024  # Replace with actual p-value\n",
        "\n",
        "    # Add the bracket and p-value annotation\n",
        "    ax.plot([x1, x2], [y, y], color='black')\n",
        "    ax.plot([x1, x1], [y - bracket_height / 2, y + bracket_height / 2], color='black')\n",
        "    ax.plot([x2, x2], [y - bracket_height / 2, y + bracket_height / 2], color='black')\n",
        "    ax.text((x1 + x2) / 2, text_y, f\"Adjusted p* = {p_value}, <0.05\", ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot boxplots and add annotations\n",
        "for ax, metric, letter in zip(axes.flatten(), metrics, subplot_letters):\n",
        "    sns.boxplot(x='Reference Systematic Review Source Database', y=metric, data=result_recall_sorted_extract, palette=custom_palette, ax=ax)\n",
        "    y_max = result_recall_sorted_extract[metric].max()\n",
        "    y_min = result_recall_sorted_extract[metric].min()\n",
        "    y_range = y_max - y_min\n",
        "    add_significance_bracket(ax, 1, 2, y_max, y_range)\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Collect handles and labels from one of the subplots (they should be the same for all)\n",
        "\n",
        "\n",
        "# Add a single legend for the entire figure\n",
        "# Create legend handles manually\n",
        "legend_elements = [Patch(facecolor='#809fff', edgecolor='#809fff', label='Cochrane'),\n",
        "                   Patch(facecolor='#70db70', edgecolor='#70db70', label='CEE'),\n",
        "                   Patch(facecolor='#ffd480', edgecolor='#ffd480', label='Campbell')]\n",
        "\n",
        "# Add a single legend for the entire figure\n",
        "fig.legend(handles=legend_elements, loc='lower center', ncol=3, title='Systematic Review Database', bbox_to_anchor=(0.5, 0.08))\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0.2, 1, 1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f75ea0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from scipy.stats import kruskal, mannwhitneyu\n",
        "\n",
        "# Initialize Markdown table rows for Kruskal-Wallis and Mann-Whitney U\n",
        "markdown_table_rows_kw = [\"| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\",\n",
        "                          \"|--------|-------------------|----------------------------|----------------------------|\"]\n",
        "\n",
        "markdown_table_rows_mwu = [\"| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value |\",\n",
        "                           \"|--------|------------|-------------|-------------|------------------|\"]\n",
        "\n",
        "# Number of Kruskal-Wallis tests being run\n",
        "num_kw_tests = 1  # Only one test per metric\n",
        "\n",
        "# New alpha after Bonferroni correction for Kruskal-Wallis\n",
        "alpha_kw = 0.05 / num_kw_tests\n",
        "\n",
        "# Loop through each metric\n",
        "for metric in ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']:\n",
        "    groups = {name: group[metric].dropna().values for name, group in result_summary_maxrecall.groupby('Reference Systematic Review Source Database')}\n",
        "    \n",
        "    if all(len(g) > 1 for g in groups.values()):\n",
        "        k_statistic, k_p_val = kruskal(*groups.values())\n",
        "        adjusted_p_val_kw = min(k_p_val * num_kw_tests, 1)\n",
        "        \n",
        "        asterisk_kw_adj = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        asterisk_kw_raw = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        \n",
        "        markdown_table_rows_kw.append(f\"| {metric} | {k_statistic:.2f} | {k_p_val:.3f}{asterisk_kw_raw} |{adjusted_p_val_kw:.3f} {asterisk_kw_adj}|\")\n",
        "        \n",
        "        if k_p_val < alpha_kw:\n",
        "            \n",
        "            mwu_results = []\n",
        "            combos = list(combinations(groups.keys(), 2))  # Convert to list to find length\n",
        "            num_combos = len(combos)  # Number of combinations\n",
        "            for grp1, grp2 in combos:\n",
        "                u_statistic, u_p_val = mannwhitneyu(groups[grp1], groups[grp2])\n",
        "                adjusted_u_p_val = min(u_p_val * num_combos, 1)  # Bonferroni correction\n",
        "               \n",
        "                asterisk_mwu_raw = \"*\" if u_p_val < 0.05 else \"\"\n",
        "                asterisk_mwu_adj = \"*\" if adjusted_u_p_val < 0.05 else \"\"\n",
        "                \n",
        "                markdown_table_rows_mwu.append(f\"| {metric} | {grp1} vs {grp2} | {u_statistic} | {u_p_val:.3f}{asterisk_mwu_raw} | {adjusted_u_p_val:.3f}{asterisk_mwu_adj} |\")\n",
        "\n",
        "# Convert lists of Markdown table rows to single strings\n",
        "markdown_table_kw_studyarea = \"\\n\".join(markdown_table_rows_kw)\n",
        "markdown_table_mwu_studyarea = \"\\n\".join(markdown_table_rows_mwu)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d444d699",
      "metadata": {},
      "source": [
        "### Seed Article Type\n",
        "\n",
        "create summary tables alongside hypothesis test results (no difference found so results are inserted into supp table, alongside box plots) - so just describe results and note that variation is not significant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8add31",
      "metadata": {},
      "outputs": [],
      "source": [
        "## this is recall > 0, seed _ num 1 but keep in mind that multiple same seeds may be used for different reviews so need to combine\n",
        "full_results_valid_recall_filter = full_results_valid_recall[~full_results_valid_recall['params.seed_id'].str.match(r\"\\['.*'\\]\")]\n",
        "full_results_valid_recall_1seed = full_results_valid_recall_filter.query('`params.seed_num` == 1').reset_index()\n",
        "full_results_valid_recall_1seed['id_cleaned'] = full_results_valid_recall_1seed['params.seed_id'].str.replace(r\"[\\'\\[\\]]\", \"\", regex=True)\n",
        "full_results_valid_recall_1seed_cleaned_oa = full_results_valid_recall_1seed[full_results_valid_recall_1seed['API'] == 'openalex'].reset_index()\n",
        "full_results_valid_recall_1seed_cleaned_ss = full_results_valid_recall_1seed[full_results_valid_recall_1seed['API'] == 'semanticscholar'].reset_index()\n",
        "\n",
        "col_interest = ['id_cleaned', 'Original Systematic Review ID', 'source_database','article_type', 'API', 'year', 'precision_auto', 'recall_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto','citations', 'references', 'citation_network_size']\n",
        "#merge with seed article data \n",
        "seed_article_all_oa_rename = seed_articles_all_oa.copy()\n",
        "seed_article_all_oa_rename.rename(columns = {'year_published' : 'year'}, inplace = True)\n",
        "seed_article_all_oa_rename['api'] = 'openalex'\n",
        "seed_articles_all_ss['api'] = 'semanticscholar'\n",
        "\n",
        "full_results_valid_recall_1seed_cleaned_oa_merge = pd.merge(full_results_valid_recall_1seed_cleaned_oa, seed_article_all_oa_rename[['seed_Id', 'api','title','year', 'article_type', 'article_subtype','source_database', 'citations', 'references', 'citation_network_size']], how='left', left_on=['id_cleaned','API'], right_on=['seed_Id', 'api'])\n",
        "full_results_valid_recall_1seed_cleaned_oa = full_results_valid_recall_1seed_cleaned_oa_merge[col_interest].copy()\n",
        "full_results_valid_recall_1seed_cleaned_ss_merge = pd.merge(full_results_valid_recall_1seed_cleaned_ss, seed_articles_all_ss[['seed_Id', 'api', 'title','year', 'article_type', 'article_subtype','source_database', 'citations', 'references','citation_network_size']], how='left', left_on=['id_cleaned','API'], right_on=['seed_Id', 'api'])\n",
        "full_results_valid_recall_1seed_cleaned_ss = full_results_valid_recall_1seed_cleaned_ss_merge[col_interest].copy()\n",
        "result_summary_maxrecall['params.seed_id'] = result_summary_maxrecall['params.seed_id'].apply(ast.literal_eval)\n",
        "\n",
        "#loop through each seed id and obtain corresponding seed article data and recall, precision, f1, f2, f3 scores\n",
        "best_seed_dict = result_summary_maxrecall[['Original Systematic Review ID', 'params.seed_id', 'API']].to_dict('records')\n",
        "\n",
        "#loop through params.seed_id and obtain corresponding seed article data\n",
        "for dct in best_seed_dict: \n",
        "    article_type_list = [] \n",
        "    recall_list = []\n",
        "    precision_list = []\n",
        "    f1_list = []\n",
        "    f2_list = []\n",
        "    f3_list = []\n",
        "    year_published = []\n",
        "\n",
        "    for id in dct['params.seed_id']:\n",
        "        original_sr_id = dct['Original Systematic Review ID']\n",
        "        api = dct['API']\n",
        "        article_type = seed_articles_all_oa.query('`seed_Id` == @id & `original_sr_id` == @original_sr_id')['article_type'].iloc[0]\n",
        "        article_type_list.append(article_type)\n",
        "        recall_list.append(full_results_valid_recall_1seed.query('`params.seed_id` == @id & `API` == @api & `Original Systematic Review ID` == @original_sr_id')['recall_auto'].iloc[0])\n",
        "        precision_list.append(full_results_valid_recall_1seed.query('`params.seed_id` == @id & `API` == @api & `Original Systematic Review ID` == @original_sr_id')['precision_auto'].iloc[0])\n",
        "        f1_list.append(full_results_valid_recall_1seed.query('`params.seed_id` == @id & `API` == @api & `Original Systematic Review ID` == @original_sr_id')['f1_score_auto'].iloc[0])\n",
        "        f2_list.append(full_results_valid_recall_1seed.query('`params.seed_id` == @id & `API` == @api & `Original Systematic Review ID` == @original_sr_id')['f2_score_auto'].iloc[0])\n",
        "        f3_list.append(full_results_valid_recall_1seed.query('`params.seed_id` == @id & `API` == @api & `Original Systematic Review ID` == @original_sr_id')['f3_score_auto'].iloc[0])\n",
        "        year_published.append(seed_articles_all_oa.query('`seed_Id` == @id & `original_sr_id` == @original_sr_id')['year_published'].iloc[0])\n",
        "    dct['article_type'] = article_type_list\n",
        "    dct['recall_auto'] = recall_list\n",
        "    dct['precision_auto'] = precision_list\n",
        "    dct['f1_score_auto'] = f1_list\n",
        "    dct['f2_score_auto'] = f2_list\n",
        "    dct['f3_score_auto'] = f3_list\n",
        "    dct['year_published'] = year_published    \n",
        "\n",
        "best_seed_df = []\n",
        "for data_dict in best_seed_dict: \n",
        "    length = len(data_dict['params.seed_id'])\n",
        "    for key, value in data_dict.items():\n",
        "        if not isinstance(value, list):\n",
        "            data_dict[key] = [value] * length\n",
        "    \n",
        "    # Convert each dictionary to a DataFrame\n",
        "    df = pd.DataFrame.from_dict(data_dict)\n",
        "    \n",
        "    # Append the DataFrame to the list\n",
        "    best_seed_df.append(df)\n",
        "    \n",
        "best_seed_df = pd.concat(best_seed_df, ignore_index = True)\n",
        "best_seed_df['count'] = 1\n",
        "\n",
        "#plot box plot of article types against recall, precision, f1, f2, f3 scores\n",
        "best_seed_type_grp = best_seed_df.groupby(['article_type']).agg({'count' : 'sum', 'recall_auto' : ['median', iqr], 'precision_auto' :['median', iqr], 'f1_score_auto' : ['median', iqr], 'f2_score_auto' : ['median', iqr], 'f3_score_auto' :['median', iqr]}).reset_index()\n",
        "best_seed_type_grp.columns = ['_'.join(col).rstrip('_') for col in best_seed_type_grp.columns.values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-automated_citation_search_perf_summary_article_type",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-automated_citation_search_perf_summary_article_type\n",
        "#| tbl-cap: 'Median (IQR) Recall, Precision, F1 score, F2 score, F3 score of the best performing seed article types'\n",
        "\n",
        "#create summary table \n",
        "best_seed_type_grp = best_seed_type_grp.sort_values(by = 'recall_auto_median', ascending=False)\n",
        "markdown_table_seed_type = \"| Article Type (n) | Median (IQR) Recall (%) | Median (IQR) Precision (%) | Median (IQR) F1 Score | Median (IQR) F2 Score | Median (IQR) F3 Score |\\n\"\n",
        "markdown_table_seed_type += \"|---|---|---|---|---|---|\\n\"\n",
        "\n",
        "for idx, row in best_seed_type_grp.iterrows():\n",
        "    markdown_table_seed_type += f\"| {row['article_type']} ({row['count_sum']}) | {row['recall_auto_median']*100:.2f} ({row['recall_auto_iqr']*100:.2f}) | {row['precision_auto_median']*100:.2f} ({row['precision_auto_iqr']*100:.2f}) | {row['f1_score_auto_median']:.3f} ({row['f1_score_auto_iqr']:.3f}) | {row['f2_score_auto_median']:.3f} ({row['f2_score_auto_iqr']:.3f}) | {row['f3_score_auto_median']:.3f} ({row['f3_score_auto_iqr']:.3f}) |\\n\"\n",
        "\n",
        "Markdown(markdown_table_seed_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48528621",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hypothesis test \n",
        "\n",
        "# Initialize Markdown table rows for Kruskal-Wallis and Mann-Whitney U\n",
        "markdown_table_rows_kw = [\"| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\",\n",
        "                          \"|--------|-------------------|----------------------------|----------------------------|\"]\n",
        "\n",
        "markdown_table_rows_mwu = [\"| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value |\",\n",
        "                           \"|--------|------------|-------------|-------------|------------------|\"]\n",
        "\n",
        "# Number of Kruskal-Wallis tests being run\n",
        "num_kw_tests = 1  # Only one test per metric\n",
        "# New alpha after Bonferroni correction for Kruskal-Wallis\n",
        "alpha_kw = 0.05 / num_kw_tests\n",
        "# Loop through each metric\n",
        "for metric in ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']:\n",
        "    groups = {name: group[metric].dropna().values for name, group in best_seed_df.groupby('article_type')}\n",
        "    \n",
        "    if all(len(g) > 1 for g in groups.values()):\n",
        "        k_statistic, k_p_val = kruskal(*groups.values())\n",
        "        adjusted_p_val_kw = min(k_p_val * num_kw_tests, 1)\n",
        "        \n",
        "        asterisk_kw_adj = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        asterisk_kw_raw = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        \n",
        "        markdown_table_rows_kw.append(f\"| {metric} | {k_statistic:.2f} | {k_p_val:.3f}{asterisk_kw_raw} |{adjusted_p_val_kw:.3f} {asterisk_kw_adj}|\")\n",
        "        \n",
        "        if k_p_val < alpha_kw:\n",
        "            \n",
        "            mwu_results = []\n",
        "            combos = list(combinations(groups.keys(), 2))  # Convert to list to find length\n",
        "            num_combos = len(combos)  # Number of combinations\n",
        "            for grp1, grp2 in combos:\n",
        "                u_statistic, u_p_val = mannwhitneyu(groups[grp1], groups[grp2])\n",
        "                adjusted_u_p_val = min(u_p_val * num_combos, 1)  # Bonferroni correction\n",
        "               \n",
        "                asterisk_mwu_raw = \"*\" if u_p_val < 0.05 else \"\"\n",
        "                asterisk_mwu_adj = \"*\" if adjusted_u_p_val < 0.05 else \"\"\n",
        "                \n",
        "                markdown_table_rows_mwu.append(f\"| {metric} | {grp1} vs {grp2} | {u_statistic} | {u_p_val:.3f}{asterisk_mwu_raw} | {adjusted_u_p_val:.3f}{asterisk_mwu_adj} |\")\n",
        "\n",
        "# Convert lists of Markdown table rows to single strings\n",
        "markdown_table_kw = \"\\n\".join(markdown_table_rows_kw)\n",
        "markdown_table_mwu = \"\\n\".join(markdown_table_rows_mwu)\n",
        "#supp tables \n",
        "markdown_table_kw_seedtype = markdown_table_kw\n",
        "markdown_table_mwu_seedtype = markdown_table_mwu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbea657",
      "metadata": {},
      "source": [
        "### Study Type Inclusion Criteria\n",
        "\n",
        "compare grey lit vs peer reviewed only - no difference - keep same format as study area\n",
        "\n",
        "*create table with summary results - and denote whether significant or not (asterisk) - raw kruskal statistic - put in supplementary info*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984f7b0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## summary results table \n",
        "result_summary_maxrecall['count'] = 1\n",
        "result_inc_criteria_summary = result_summary_maxrecall.groupby(['included_study_type_selection_criteria']).agg({\n",
        "    'count' : 'sum',\n",
        "    'recall_auto' : ['median', iqr],\n",
        "    'precision_auto' : ['median', iqr],\n",
        "    'f1_score_auto' : ['median', iqr],\n",
        "    'f2_score_auto' : ['median', iqr],\n",
        "    'f3_score_auto' : ['median', iqr]\n",
        "}).reset_index() \n",
        "result_inc_criteria_summary.columns = ['_'.join(col).rstrip('_') for col in result_inc_criteria_summary.columns.values]\n",
        "result_inc_criteria_summary.sort_values(by=['recall_auto_median'], ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-automated_citation_search_perf_summary_inc_criteria",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-automated_citation_search_perf_summary_inc_criteria\n",
        "#| tbl-cap: 'Median (IQR) Recall, Precision, F1 score, F2 score, F3 score, by study inclusion criteria'\n",
        "\n",
        "markdown_table_inc_criteria = \"| Inclusion Criteria (n) | Median (IQR) Recall (%) | Median (IQR) Precision (%) | Median (IQR) F1 Score | Median (IQR) F2 Score | Median (IQR) F3 Score |\\n\"\n",
        "markdown_table_inc_criteria += \"|---|---|---|---|---|---|\\n\"\n",
        "\n",
        "for idx, row in result_inc_criteria_summary.iterrows():\n",
        "    markdown_table_inc_criteria += f\"| {row['included_study_type_selection_criteria']} ({row['count_sum']}) | {row['recall_auto_median']*100:.2f} ({row['recall_auto_iqr']*100:.2f}) | {row['precision_auto_median']*100:.2f} ({row['precision_auto_iqr']*100:.2f}) | {row['f1_score_auto_median']:.3f} ({row['f1_score_auto_iqr']:.3f}) | {row['f2_score_auto_median']:.3f} ({row['f2_score_auto_iqr']:.3f}) | {row['f3_score_auto_median']:.3f} ({row['f3_score_auto_iqr']:.3f}) |\\n\"\n",
        "\n",
        "Markdown(markdown_table_inc_criteria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20a7359",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Markdown table rows for Kruskal-Wallis and Mann-Whitney U\n",
        "markdown_table_rows_kw = [\"| Metric | Kruskal Statistic |  Kruskal p-value (Raw) | Kruskal p-value (Adjusted) |\",\n",
        "                          \"|--------|-------------------|----------------------------|----------------------------|\"]\n",
        "\n",
        "markdown_table_rows_mwu = [\"| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value |\",\n",
        "                           \"|--------|------------|-------------|-------------|------------------|\"]\n",
        "\n",
        "# Number of Kruskal-Wallis tests being run\n",
        "num_kw_tests = 1  # Only one test per metric\n",
        "\n",
        "# New alpha after Bonferroni correction for Kruskal-Wallis\n",
        "alpha_kw = 0.05 / num_kw_tests\n",
        "\n",
        "# Loop through each metric\n",
        "for metric in ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']:\n",
        "    groups = {name: group[metric].dropna().values for name, group in result_summary_maxrecall.groupby('included_study_type_selection_criteria')}\n",
        "    \n",
        "    if all(len(g) > 1 for g in groups.values()):\n",
        "        k_statistic, k_p_val = kruskal(*groups.values())\n",
        "        adjusted_p_val_kw = min(k_p_val * num_kw_tests, 1)\n",
        "        \n",
        "        asterisk_kw_adj = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        asterisk_kw_raw = \"*\" if adjusted_p_val_kw < 0.05 else \"\"\n",
        "        \n",
        "        markdown_table_rows_kw.append(f\"| {metric} | {k_statistic:.2f} | {k_p_val:.3f}{asterisk_kw_raw} |{adjusted_p_val_kw:.3f} {asterisk_kw_adj}|\")\n",
        "\n",
        "        if k_p_val < alpha_kw:\n",
        "            \n",
        "            mwu_results = []\n",
        "            combos = list(combinations(groups.keys(), 2))  # Convert to list to find length\n",
        "            num_combos = len(combos)  # Number of combinations\n",
        "            for grp1, grp2 in combos:\n",
        "                u_statistic, u_p_val = mannwhitneyu(groups[grp1], groups[grp2])\n",
        "                adjusted_u_p_val = min(u_p_val * num_combos, 1)  # Bonferroni correction\n",
        "               \n",
        "                asterisk_mwu_raw = \"*\" if u_p_val < 0.05 else \"\"\n",
        "                asterisk_mwu_adj = \"*\" if adjusted_u_p_val < 0.05 else \"\"\n",
        "                \n",
        "                markdown_table_rows_mwu.append(f\"| {metric} | {grp1} vs {grp2} | {u_statistic} | {u_p_val:.3f}{asterisk_mwu_raw} | {adjusted_u_p_val:.3f}{asterisk_mwu_adj} |\")\n",
        "\n",
        "# Convert lists of Markdown table rows to single strings\n",
        "markdown_table_kw = \"\\n\".join(markdown_table_rows_kw)\n",
        "markdown_table_mwu = \"\\n\".join(markdown_table_rows_mwu)\n",
        "markdown_table_inc_criteira_kw = markdown_table_kw\n",
        "markdown_table_inc_criteria_mwu = markdown_table_mwu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60dd92d2",
      "metadata": {},
      "source": [
        "### API Choice\n",
        "\n",
        "Hypothesis test (mann whitney u) - no difference + symmart tabe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbef1b7d",
      "metadata": {
        "tbl-label": "performance_apicomparison_summary"
      },
      "outputs": [],
      "source": [
        "#| tbl-cap: 'Median (IQR) Recall, Precision, F1 Score, F2 score and F3 score of the best performing automated citation searching runs from each API'\n",
        "#summary table \n",
        "\n",
        "\n",
        "result_best_performing_oa = (\n",
        "    full_results_valid_recall[full_results_valid_recall['API'] == 'openalex']\n",
        "    .sort_values(by=['Original Systematic Review ID', 'recall_auto', 'f3_score_auto'], ascending=[True, False, False])\n",
        "    .groupby('Original Systematic Review ID')\n",
        "    .first()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "result_best_performing_ss = (    full_results_valid_recall[full_results_valid_recall['API'] == 'semanticscholar']\n",
        "    .sort_values(by=['Original Systematic Review ID', 'recall_auto', 'f3_score_auto'], ascending=[True, False, False])\n",
        "    .groupby('Original Systematic Review ID')\n",
        "    .first()\n",
        "    .reset_index()\n",
        "                            )\n",
        "result_best_performing_bothapi = pd.concat([result_best_performing_oa, result_best_performing_ss], ignore_index=True)\n",
        "\n",
        "result_best_performing_bothapi_summary = result_best_performing_bothapi.groupby('API').agg(\n",
        "    {\n",
        "        'recall_auto' : ['median', iqr],\n",
        "        'precision_auto' : ['median', iqr],\n",
        "        'f1_score_auto' : ['median', iqr],\n",
        "        'f2_score_auto' : ['median', iqr],\n",
        "        'f3_score_auto' : ['median', iqr],\n",
        "    }\n",
        ").reset_index()\n",
        "\n",
        "#flatten column names and structure as markdown table \n",
        "result_best_performing_bothapi_summary.columns = ['_'.join(col).rstrip('_') for col in result_best_performing_bothapi_summary.columns.values]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-automated_citation_search_perf_summary_api",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-automated_citation_search_perf_summary_api\n",
        "#| tbl-cap: 'Median (IQR) Recall, Precision, F1 Score, F2 score and F3 score of the best performing automated citation searching runs from each API'\n",
        "#summary table \n",
        "\n",
        "markdown_table_api = \"| API | Median (IQR) Recall (%) | Median (IQR) Precision (%) | Median (IQR) F1 Score | Median (IQR) F2 Score | Median (IQR) F3 Score |\\n\"\n",
        "markdown_table_api += \"|---|---|---|---|---|---|\\n\"\n",
        "\n",
        "for idx, row in result_best_performing_bothapi_summary.iterrows():\n",
        "    markdown_table_api += f\"| {row['API']} | {row['recall_auto_median']*100:.2f} ({row['recall_auto_iqr']*100:.2f}) | {row['precision_auto_median']*100:.2f} ({row['precision_auto_iqr']*100:.2f}) | {row['f1_score_auto_median']:.3f} ({row['f1_score_auto_iqr']:.3f}) | {row['f2_score_auto_median']:.3f} ({row['f2_score_auto_iqr']:.3f}) | {row['f3_score_auto_median']:.3f} ({row['f3_score_auto_iqr']:.3f}) |\\n\"\n",
        "\n",
        "Markdown(markdown_table_api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05a71f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a list to hold Markdown table rows\n",
        "markdown_table_rows = [\"| Metric | Comparison | U Statistic | Raw p-value | Adjusted p-value | Significant |\",\n",
        "                       \"|--------|------------|-------------|-------------|------------------|-------------|\"]\n",
        "\n",
        "# Initialize alpha for Bonferroni correction\n",
        "alpha = 0.05\n",
        "\n",
        "for metric in ['recall_auto', 'precision_auto', 'f1_score_auto', 'f2_score_auto', 'f3_score_auto']:\n",
        "    groups = {name: group[metric].dropna().values for name, group in result_best_performing_bothapi.groupby('API')}\n",
        "    \n",
        "    # Run Mann-Whitney U test for pairwise comparisons\n",
        "    if all(len(g) > 1 for g in groups.values()):\n",
        "        mwu_results = []\n",
        "        combos = combinations(groups.keys(), 2)\n",
        "        for grp1, grp2 in combos:\n",
        "            u_statistic, u_p_val = mannwhitneyu(groups[grp1], groups[grp2])\n",
        "            mwu_results.append((grp1, grp2, u_statistic, u_p_val))\n",
        "        \n",
        "        # Bonferroni Correction\n",
        "        num_comparisons = len(mwu_results)\n",
        "        corrected_alpha = alpha / num_comparisons\n",
        "        \n",
        "        for grp1, grp2, u_statistic, u_p_val in mwu_results:\n",
        "            adjusted_p_val = min(u_p_val * num_comparisons, 1)\n",
        "            significance_str = \"*\" if adjusted_p_val < corrected_alpha else \"\"\n",
        "            markdown_table_rows.append(f\"| {metric} | {grp1} vs {grp2} | {u_statistic:.2f} | {u_p_val:.3f} | {adjusted_p_val:.3f}{significance_str} | {significance_str} |\")\n",
        "\n",
        "# Convert list of Markdown table rows to a single string\n",
        "markdown_table_api_kw = \"\\n\".join(markdown_table_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8b8dfa",
      "metadata": {},
      "source": [
        "\n",
        "# Extra Supplementary Tables\n",
        "\n",
        "**Table S1 : Sample Systematic Review Characteristics (Title, Source Database, Publication Year, Search Strategy, Study Type Inclusion Criteria)**\n",
        "\n",
        "| \\#  | Title                                                                                                                                                                                        | Source Database  | Publication Year | Search Strategy                                                                                | Study Type Inclusion Criteria             |\n",
        "|:----------:|------------|------------|------------|------------|------------|\n",
        "|  1  | Mechanisms of Impact of Blue Spaces on Human Health: A Systematic Literature Review and Meta-Analysis                                                                                        | CEEDER           | 2021             | boolean keyword + backwards citation search                                                    | peer-reviewed literature                  |\n",
        "|  2  | A systematic review of the socio-economic impacts of large-scale tree plantations, worldwide                                                                                                 | CEEDER           | 2018             | boolean keyword + backwards citation search + handsearch                                       | peer-reviewed literature, grey literature |\n",
        "|  3  | Are small protected habitat patches within boreal production forests effective in conserving species richness, abundance and community composition? A systematic review                      | CEEDER           | 2021             | boolean keyword + full citation search + handsearch + expert consultation                      | peer-reviewed literature, grey literature |\n",
        "|  4  | What is the effect of phasing out long-chain per- and polyfluoroalkyl substances on the concentrations of perfluoroalkyl acids and their precursors in the environment? A systematic review  | CEEDER           | 2018             | boolean keyword + handsearch + expert consultation                                             | peer-reviewed literature, grey literature |\n",
        "|  5  | How does roadside vegetation management affect the diversity of vascular plants and invertebrates? A systematic review                                                                       | CEEDER           | 2018             | boolean keyword + prior evidence map                                                           | peer-reviewed literature, grey literature |\n",
        "|  6  | The effectiveness of non-native fish removal techniques in freshwater ecosystems: a systematic review                                                                                        | CEEDER           | 2019             | boolean keyword + backwards citation search + handsearch + expert consultation + crowdsourcing | peer-reviewed literature, grey literature |\n",
        "|  7  | Impacts of dead wood manipulation on the biodiversity of temperate and boreal forests. A systematic review                                                                                   | CEEDER           | 2019             | boolean keyword + handsearch + prior evidence map                                              | peer-reviewed literature, grey literature |\n",
        "|  8  | The Different Dimensions of Livelihood Impacts of Payments for Environmental Services (PES) Schemes: A Systematic Review                                                                     | CEEDER           | 2018             | boolean keyword                                                                                | peer-reviewed literature, grey literature |\n",
        "|  9  | The effectiveness of spawning habitat creation or enhancement for substrate-spawning temperate fish: a systematic review                                                                     | CEEDER           | 2019             | boolean keyword + backwards citation search + handsearch + crowdsourcing                       | peer-reviewed literature, grey literature |\n",
        "| 10  | What are the effects of even-aged and uneven-aged forest management on boreal forest biodiversity in Fennoscandia and European Russia? A systematic review                                   | CEEDER           | 2021             | boolean keyword + full citation search + handsearch                                            | peer-reviewed literature, grey literature |\n",
        "| 11  | Strengthening women's empowerment and gender equality in fragile contexts towards peaceful and inclusive societies: A systematic review and metaโanalysis                                    | Campbell Reviews | 2022             | boolean keyword + full citation search + handsearch                                            | peer-reviewed literature, grey literature |\n",
        "| 12  | Red light camera interventions for reducing traffic violations and traffic crashes: A systematic review                                                                                      | Campbell Reviews | 2020             | boolean keyword + backward citation search + handsearch                                        | peer-reviewed literature, grey literature |\n",
        "| 13  | Aquaculture for improving productivity, income, nutrition and women's empowerment in lowโ and middleโincome countries: A systematic review and metaโanalysis                                 | Campbell Reviews | 2021             | boolean keyword + full citation search + handsearch                                            | peer-reviewed literature, grey literature |\n",
        "| 14  | Policies and interventions to remove genderโrelated barriers to girls' school participation and learning in lowโ and middleโincome countries: A systematic review of the evidence            | Campbell Reviews | 2022             | boolean keyword + backwards citation search + handsearch                                       | peer-reviewed literature, grey literature |\n",
        "| 15  | Citizen engagement in public services in lowโ and middleโincome countries: A mixedโmethods systematic review of participation, inclusion, transparency and accountability (PITA) initiatives | Campbell Reviews | 2019             | boolean keyword + backwards citation search + handsearch                                       | peer-reviewed literature, grey literature |\n",
        "| 16  | The impacts of agroforestry interventions on agricultural productivity, ecosystem services, and human wellโbeing in lowโ and middleโincome countries: A systematic review                    | Campbell Reviews | 2021             | boolean keyword + full citation search + handsearch + prior evidence map                       | peer-reviewed literature, grey literature |\n",
        "| 17  | Multiagency programs with police as a partner for reducing radicalisation to violence                                                                                                        | Campbell Reviews | 2021             | boolean keyword + full citation search + expert consultation                                   | peer-reviewed literature, grey literature |\n",
        "| 18  | Adult/child ratio and group size in early childhood education or care to promote the development of children aged 0--5 years: A systematic review                                            | Campbell Reviews | 2022             | boolean keyword + full citation search + handsearch + expert consultation                      | peer-reviewed literature, grey literature |\n",
        "| 19  | Interventions for improving executive functions in children with foetal alcohol spectrum disorder (FASD): A systematic review                                                                | Campbell Reviews | 2022             | boolean keyword + full citation search + handsearch + expert consultation                      | peer-reviewed literature, grey literature |\n",
        "| 20  | Selective serotonin reuptake inhibitors (SSRIs) for stroke recovery                                                                                                                          | CDSR             | 2021             | boolean keyword + backwards citation search + expert consultation                              | peer-reviewed literature                  |\n",
        "| 21  | Vena caval filters for the prevention of pulmonary embolism                                                                                                                                  | CDSR             | 2020             | boolean keyword + forward citation + previous version of review                                | peer-reviewed literature                  |\n",
        "| 22  | Atovaquone-proguanil for treating uncomplicated Plasmodium falciparum malaria                                                                                                                | CDSR             | 2021             | boolean keyword + backwards citation search + previous version of review                       | peer-reviewed literature                  |\n",
        "| 23  | Pharmaceutical policies: effects of regulating drug insurance schemes                                                                                                                        | CDSR             | 2022             | boolean keyword + backwards citation search + expert consultation                              | peer-reviewed literature, grey literature |\n",
        "| 24  | Ab interno supraciliary microstent surgery for open-angle glaucoma                                                                                                                           | CDSR             | 2021             | boolean keyword + backwards citation search + handsearch                                       | peer-reviewed literature                  |\n",
        "| 25  | Probiotics for the prevention of Hirschsprung-associated enterocolitis                                                                                                                       | CDSR             | 2022             | boolean keyword + backwards citation search + expert consultation                              | peer-reviewed literature                  |\n",
        "| 26  | Pentoxifylline for the treatment of endometriosis-associated pain and infertility                                                                                                            | CDSR             | 2021             | boolean keyword + backwards citation search + expert consultation                              | peer-reviewed literature                  |\n",
        "| 27  | How effects on health equity are assessed in systematic reviews of interventions                                                                                                             | CDSR             | 2022             | boolean keyword + backwards citation search + expert consultation + previous version of review | peer-reviewed literature, grey literature |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-theoretical_max_recall_raw",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-theoretical_max_recall_raw\n",
        "#| tbl-cap: 'Perentage of Included Articles with Retrieval IDs, and corresponding theoretical maximum achievable recall for each API (OpenAlex and Semantic Scholar), for each Systematic Review'\n",
        "\n",
        "\n",
        "##supplementary table\n",
        "\n",
        "def calculate_retrieval_stats(df, original_sr_id, included_num):\n",
        "    stats = {}\n",
        "    mask = df['original_sr_id'] == original_sr_id\n",
        "    filtered_df = df[mask]\n",
        "\n",
        "    stats['no_id_num'] = filtered_df['no_id'].sum()\n",
        "    stats['no_data_from_api_num'] = filtered_df['no_data_from_api'].sum()\n",
        "    stats['no_id_no_api_num'] = filtered_df['no_data_no_id'].sum()\n",
        "    stats['api_success_retrieved_num'] = sum(filtered_df['no_data_no_id'] == 0)\n",
        "    stats['retrievable_id_num'] = included_num - stats['no_id_num']\n",
        "    \n",
        "\n",
        "    if included_num != 0:\n",
        "        stats['no_id_percentage'] = round((stats['no_id_num'] / included_num) * 100, 1)\n",
        "        stats['no_data_api_percentage'] = round((stats['no_data_from_api_num'] / included_num) * 100, 1)\n",
        "        stats['api_success_retrieved_percentage'] = round((stats['api_success_retrieved_num'] / included_num) * 100, 1)\n",
        "        stats['retrievable_id_percentage'] = round((stats['retrievable_id_num'] / included_num) * 100, 1)\n",
        "\n",
        "        stats['no_id_percentage_with_counts'] = f\"{stats['no_id_percentage']} % (n={stats['no_id_num']})\"\n",
        "        stats['retrievable_id_percentage_with_counts'] = f\"{stats['retrievable_id_percentage']} % (n={stats['retrievable_id_num']})\"\n",
        "        stats['no_data_api_percentage_with_counts'] = f\"{stats['no_data_api_percentage']} % (n={stats['no_data_from_api_num']})\"\n",
        "        stats['success_retrieved_percentage_with_counts'] = f\"{stats['api_success_retrieved_percentage']} % (n={stats['api_success_retrieved_num']})\"\n",
        "    else:\n",
        "        stats['no_id_percentage'] = 0\n",
        "        stats['no_data_api_percentage'] = 0\n",
        "        stats['success_retrieved_percentage'] = 0\n",
        "        stats['no_id_percentage_with_counts'] = f\"{stats['no_id_percentage']}% (n={stats['no_id_num']})\"\n",
        "        stats['no_data_api_percentage_with_counts'] = f\"{stats['no_data_api_percentage']}% (n={stats['no_data_from_api_num']})\"\n",
        "        stats['success_retrieved_percentage_with_counts'] = f\"{stats['success_retrieved_percentage']}% (n={stats['success_retrieved_num']})\"\n",
        "        stats['retrievable_id_percentage_with_counts'] = f\"{stats['retrievable_id_percentage']}% (n={stats['retrievable_id_num']})\"\n",
        "\n",
        "    return stats\n",
        "\n",
        "original_sr_all = baseline_review_data_oa.copy()\n",
        "\n",
        "for original_sr_id in original_sr_all['id'].unique():\n",
        "    mask_oa = included_all_oa['original_sr_id'] == original_sr_id\n",
        "    mask_ss = included_all_ss['original_sr_id'] == original_sr_id\n",
        "    included_num = len(included_all_oa[mask_oa])\n",
        "\n",
        "    stats_oa = calculate_retrieval_stats(included_all_oa, original_sr_id, included_num)\n",
        "    stats_ss = calculate_retrieval_stats(included_all_ss, original_sr_id, included_num)\n",
        "\n",
        "    original_sr_all.loc[original_sr_all['id'] == original_sr_id, 'included_num'] = included_num\n",
        "\n",
        "    for prefix, stats in {'oa': stats_oa, 'ss': stats_ss}.items():\n",
        "        for key, value in stats.items():\n",
        "            original_sr_all.loc[original_sr_all['id'] == original_sr_id, f\"{key}_{prefix}\"] = value\n",
        "\n",
        "extract_col = ['source_database', 'included_num', 'retrievable_id_percentage_with_counts_ss','success_retrieved_percentage_with_counts_oa','success_retrieved_percentage_with_counts_ss']\n",
        "original_sr_performance_df = original_sr_all[extract_col].copy()\n",
        "original_sr_performance_df.rename(columns = {'retrievable_id_percentage_with_counts_ss':'retrievable_id_percentage_with_counts'}, inplace = True)\n",
        "\n",
        "extract_col_new = ['source_database', 'included_num', 'retrievable_id_percentage_with_counts','success_retrieved_percentage_with_counts_oa','success_retrieved_percentage_with_counts_ss']\n",
        "\n",
        "## change column names \n",
        "target_col_names = ['Source Database', 'Number of Included Articles', 'Percentage of Included Articles with Retrievable IDs', 'Maximum Theoretical Recall (OpenAlex)', 'Maximum Theoretical Recall (Semantic Scholar)']\n",
        "col_name_mapping = dict(zip(extract_col_new, target_col_names))\n",
        "original_sr_performance_df.rename(columns = col_name_mapping, inplace=True)\n",
        "\n",
        "\n",
        "# Generate Markdown; the string formatting should be preserved\n",
        "markdown_table = original_sr_performance_df.to_markdown()\n",
        "\n",
        "# # Convert DataFrame back to original dtypes\n",
        "# for col in baseline_review_data_oa.columns:\n",
        "#     baseline_review_data_oa[col] = baseline_review_data_oa[col].astype(original_dtypes[col])\n",
        "\n",
        "Markdown(markdown_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-kw_test_studyarea",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-kw_test_studyarea\n",
        "#| tbl-cap: 'Kruskal Wallis test results for Study Area vs Performance (Recall, Precision, F1 Score, F2 Score, F3 Score)'\n",
        "\n",
        "Markdown(markdown_table_kw_studyarea)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-mwu_test_studyarea",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-mwu_test_studyarea\n",
        "#| tbl-cap: 'Mann Whitney U test results for Study Area Pairwise Comparisons vs Performance (Recall, Precision, F1 Score, F2 Score, F3 Score)'\n",
        "Markdown(markdown_table_mwu_studyarea)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-kw_test_seedtype",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-kw_test_seedtype\n",
        "#| tbl-cap: 'Kruskal Wallis test results for Seed Article Types vs Performance (Recall, Precision, F1 Score, F2 Score, F3 Score)'\n",
        "Markdown(markdown_table_kw_seedtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-seed_perf-precisionrecall",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-seed_perf-precisionrecall\n",
        "#| fig-cap: Box plot of various seed article types vs recall (A) and precision (B). Only seed article types with n>5 are plotted.\n",
        "#plot box plot \n",
        "best_seed_df_filter_samplesize = best_seed_df.query('article_type not in  [\"consensus article\", \"framework\"]')\n",
        "\n",
        "# Create the 2x1 Plot grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
        "# Add subplot letters\n",
        "subplot_letters = ['A', 'B']\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot for Recall\n",
        "sns.boxplot(x='article_type', \n",
        "            y='recall_auto', \n",
        "            data=best_seed_df_filter_samplesize, \n",
        "            ax=axes[0])\n",
        "axes[0].legend().remove()\n",
        "axes[0].set_xlabel('Seed Article Type')\n",
        "plt.xticks(rotation=45)\n",
        "# Plot for Precision\n",
        "sns.boxplot(x='article_type', \n",
        "            y='precision_auto', \n",
        "            data=best_seed_df_filter_samplesize, \n",
        "            ax=axes[1])\n",
        "axes[1].legend().remove() \n",
        "axes[1].set_xlabel('Seed Article Type')\n",
        "for ax in axes:\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjusted the tight_layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-seed_perf-fscore",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-seed_perf-fscore\n",
        "#| fig-cap: Box plot of various seed article types vs F-1 Score (A) F2 Score (B) and (C) F3 Score. Only seed article types with n>5 are plotted.\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "subplot_letters = ['A', 'B', 'C']\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot for F1 Score\n",
        "sns.boxplot(x='article_type', \n",
        "            y='f1_score_auto', \n",
        "            data=best_seed_df_filter_samplesize, \n",
        "            ax=axes[0])\n",
        "axes[0].set_xlabel('Seed Article Type')\n",
        "\n",
        "# Plot for F2 Score\n",
        "sns.boxplot(x='article_type', \n",
        "            y='f2_score_auto', \n",
        "            data=best_seed_df_filter_samplesize, \n",
        "            ax=axes[1])\n",
        "axes[1].set_xlabel('Seed Article Type')\n",
        "\n",
        "# Plot for F3 Score\n",
        "sns.boxplot(x='article_type', \n",
        "            y='f3_score_auto', \n",
        "            data=best_seed_df_filter_samplesize, \n",
        "            ax=axes[2])\n",
        "axes[2].set_xlabel('Seed Article Type')\n",
        "for ax in axes:\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-kw_test_inc_criteria",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-kw_test_inc_criteria\n",
        "#| tbl-cap: 'Kruskal Wallis test results for Inclusion Criteria Types vs Performance (Recall, Precision, F1 Score, F2 Score, F3 Score)'\n",
        "Markdown(markdown_table_inc_criteira_kw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-peerreviewgrey-recallprecision",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-peerreviewgrey-recallprecision\n",
        "#| fig-cap: 'Boxplot of (A) Recall and (B) Precision, by study type inclusion criteria'\n",
        "\n",
        "\n",
        "# Create the 2x1 Plot grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
        "# Add subplot letters\n",
        "subplot_letters = ['A', 'B']\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot for Recall\n",
        "sns.boxplot(x='included_study_type_selection_criteria', \n",
        "            y='recall_auto', \n",
        "            data=result_summary_maxrecall, \n",
        "            ax=axes[0])\n",
        "axes[0].legend().remove()\n",
        "axes[0].set_xlabel('Included Study Selection Criteria')\n",
        "\n",
        "# Plot for Precision\n",
        "sns.boxplot(x='included_study_type_selection_criteria', \n",
        "            y='precision_auto', \n",
        "            data=result_summary_maxrecall, \n",
        "            ax=axes[1])\n",
        "axes[1].legend().remove() \n",
        "axes[1].set_xlabel('Included Study Selection Criteria')\n",
        "for ax in axes:\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjusted the tight_layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-peerreviewgrey-fscore",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-peerreviewgrey-fscore\n",
        "#| fig-cap: 'Boxplot of F1 score, F2 score and F3 score by study type inclusion criteria'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "subplot_letters = ['A', 'B', 'C']\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot for F1 Score\n",
        "sns.boxplot(x='included_study_type_selection_criteria', \n",
        "            y='f1_score_auto', \n",
        "            data=result_summary_maxrecall, \n",
        "            ax=axes[0])\n",
        "axes[0].set_xlabel('Included Study Selection Criteria')\n",
        "\n",
        "# Plot for F2 Score\n",
        "sns.boxplot(x='included_study_type_selection_criteria', \n",
        "            y='f2_score_auto', \n",
        "            data=result_summary_maxrecall, \n",
        "            ax=axes[1])\n",
        "axes[1].set_xlabel('Included Study Selection Criteria')\n",
        "\n",
        "# Plot for F3 Score\n",
        "sns.boxplot(x='included_study_type_selection_criteria', \n",
        "            y='f3_score_auto', \n",
        "            data=result_summary_maxrecall, \n",
        "            ax=axes[2])\n",
        "for ax in axes:\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "\n",
        "axes[2].set_xlabel('Included Study Selection Criteria')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-kw_test_api",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: tbl-kw_test_api\n",
        "#| tbl-cap: 'Kruskal Wallis test results for API Choice vs Performance (Recall, Precision, F1 Score, F2 Score, F3 Score)'\n",
        "\n",
        "Markdown(markdown_table_api_kw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-api-recallprecision",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-api-recallprecision\n",
        "#| fig-cap: Boxplot of (A) Recall and (B) Precision of the best performing automated citation searching runs for each API\n",
        "\n",
        "# Create the 2x1 Plot grid\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "# Add subplot letters\n",
        "subplot_letters = ['A', 'B']\n",
        "custom_palette_api = {'openalex': '#809fff',  # Light blue\n",
        "                  'semanticscholar': '#70db70',}\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "\n",
        "# Plot for Recall\n",
        "sns.boxplot(x='API', \n",
        "            y='recall_auto', \n",
        "            data=result_best_performing_bothapi, \n",
        "            palette = custom_palette_api,\n",
        "            ax=axes[0])\n",
        "axes[0].set_xlabel('API')\n",
        "\n",
        "# Plot for Precision\n",
        "sns.boxplot(x='API', \n",
        "            y='precision_auto', \n",
        "            data=result_best_performing_bothapi, \n",
        "            palette = custom_palette_api,\n",
        "            ax=axes[1])\n",
        "axes[1].set_xlabel('API')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fig-performance-comparison-api-fscore",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| label: fig-performance-comparison-api-fscore\n",
        "#| fig-cap: 'Boxplot of (A) F1 Score, (B) F2 Score, and (C) F3 Score of the best performing automated citation searching runs for each API'\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "subplot_letters = ['A', 'B', 'C']\n",
        "\n",
        "for ax, letter in zip(axes.flatten(), subplot_letters):\n",
        "    ax.text(0, 1.05, letter, transform=ax.transAxes, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot for F1 Score\n",
        "sns.boxplot(x='API', \n",
        "            y='f1_score_auto', \n",
        "            data=result_best_performing_bothapi, \n",
        "            palette = custom_palette_api,\n",
        "            ax=axes[0])\n",
        "axes[0].set_xlabel('API')\n",
        "\n",
        "# Plot for F2 Score\n",
        "sns.boxplot(x='API', \n",
        "            y='f2_score_auto', \n",
        "            data=result_best_performing_bothapi, \n",
        "            palette = custom_palette_api,\n",
        "            ax=axes[1])\n",
        "axes[1].set_xlabel('API')\n",
        "\n",
        "# Plot for F3 Score\n",
        "sns.boxplot(x='API', \n",
        "            y='f3_score_auto', \n",
        "            data=result_best_performing_bothapi, \n",
        "            palette = custom_palette_api,\n",
        "            ax=axes[2])\n",
        "axes[2].set_xlabel('API')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
